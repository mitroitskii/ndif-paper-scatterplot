row_id,rank,method,method_short,method_details,evaluation_date,metrics,raw_metrics,uses_additional_data,paper,tags,hellaswag_accuracy
119129,1,CompassMTL 567M with Tailor,CompassMTL 567M with Tailor,,2022-10-12,{'Accuracy': '96.1'},{'Accuracy': 96.1},False,"{'id': 1091660, 'title': 'Task Compass: Scaling Multi-task Pre-training with Task Prefix', 'url': '/paper/task-compass-scaling-multi-task-pre-training', 'published': '2022-10-12T00:00:00.000000', 'code': True, 'review_url': '/paper/task-compass-scaling-multi-task-pre-training/review/?hl=119129'}",[],96.1
119125,2,CompassMTL 567M,CompassMTL 567M,,2022-10-12,{'Accuracy': '95.6'},{'Accuracy': 95.6},False,"{'id': 1091660, 'title': 'Task Compass: Scaling Multi-task Pre-training with Task Prefix', 'url': '/paper/task-compass-scaling-multi-task-pre-training', 'published': '2022-10-12T00:00:00.000000', 'code': True, 'review_url': '/paper/task-compass-scaling-multi-task-pre-training/review/?hl=119125'}",[],95.6
119136,3,DeBERTa-Large 304M (classification-based),DeBERTa-Large 304M ,classification-based,2022-10-29,{'Accuracy': '95.6'},{'Accuracy': 95.6},False,"{'id': 1102735, 'title': 'Two is Better than Many? Binary Classification as an Effective Approach to Multi-Choice Question Answering', 'url': '/paper/two-is-better-than-many-binary-classification', 'published': '2022-10-29T00:00:00.000000', 'code': True, 'review_url': '/paper/two-is-better-than-many-binary-classification/review/?hl=119136'}",[],95.6
99240,4,GPT-4 (10-shot),GPT-4 ,10-shot,2023-03-15,{'Accuracy': '95.3'},{'Accuracy': 95.3},False,"{'id': 1174373, 'title': 'GPT-4 Technical Report', 'url': '/paper/gpt-4-technical-report-1', 'published': '2023-03-15T00:00:00.000000', 'code': True, 'review_url': '/paper/gpt-4-technical-report-1/review/?hl=99240'}",[],95.3
123793,5,LLaMA3+MoSLoRA,LLaMA3+MoSLoRA,,2024-06-16,{'Accuracy': '95.0'},{'Accuracy': 95.0},False,"{'id': 1466105, 'title': 'Mixture-of-Subspaces in Low-Rank Adaptation', 'url': '/paper/mixture-of-subspaces-in-low-rank-adaptation', 'published': '2024-06-16T00:00:00.000000', 'code': True, 'review_url': '/paper/mixture-of-subspaces-in-low-rank-adaptation/review/?hl=123793'}",[],95.0
119133,6,DeBERTa-Large 304M,DeBERTa-Large 304M,,2022-10-29,{'Accuracy': '94.7'},{'Accuracy': 94.7},False,"{'id': 1102735, 'title': 'Two is Better than Many? Binary Classification as an Effective Approach to Multi-Choice Question Answering', 'url': '/paper/two-is-better-than-many-binary-classification', 'published': '2022-10-29T00:00:00.000000', 'code': True, 'review_url': '/paper/two-is-better-than-many-binary-classification/review/?hl=119133'}",[],94.7
122594,7,LLaMA-2 13B + MixLoRA,LLaMA-2 13B + MixLoRA,,2024-04-22,{'Accuracy': '94.7'},{'Accuracy': 94.7},False,"{'id': 1426089, 'title': 'MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts', 'url': '/paper/mixlora-enhancing-large-language-models-fine', 'published': '2024-04-22T00:00:00.000000', 'code': True, 'review_url': '/paper/mixlora-enhancing-large-language-models-fine/review/?hl=122594'}",[],94.7
117836,8,Unicorn 11B (fine-tuned),Unicorn 11B ,fine-tuned,2021-03-24,{'Accuracy': '93.9'},{'Accuracy': 93.9},True,"{'id': 756835, 'title': 'UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark', 'url': '/paper/unicorn-on-rainbow-a-universal-commonsense', 'published': '2021-03-24T00:00:00.000000', 'code': True, 'review_url': '/paper/unicorn-on-rainbow-a-universal-commonsense/review/?hl=117836'}",[],93.9
122584,9,LLaMA-3 8B + MixLoRA,LLaMA-3 8B + MixLoRA,,2024-04-22,{'Accuracy': '93.3'},{'Accuracy': 93.3},False,"{'id': 1426089, 'title': 'MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts', 'url': '/paper/mixlora-enhancing-large-language-models-fine', 'published': '2024-04-22T00:00:00.000000', 'code': True, 'review_url': '/paper/mixlora-enhancing-large-language-models-fine/review/?hl=122584'}",[],93.3
122576,10,LLaMA-2 7B + MixLoRA,LLaMA-2 7B + MixLoRA,,2024-04-22,{'Accuracy': '93.1'},{'Accuracy': 93.1},False,"{'id': 1426089, 'title': 'MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts', 'url': '/paper/mixlora-enhancing-large-language-models-fine', 'published': '2024-04-22T00:00:00.000000', 'code': True, 'review_url': '/paper/mixlora-enhancing-large-language-models-fine/review/?hl=122576'}",[],93.1
119147,11,DeBERTa++,DeBERTa++,,2020-06-05,{'Accuracy': '93'},{'Accuracy': 93.0},False,"{'id': 201217, 'title': 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention', 'url': '/paper/deberta-decoding-enhanced-bert-with', 'published': '2020-06-05T00:00:00.000000', 'code': True, 'review_url': '/paper/deberta-decoding-enhanced-bert-with/review/?hl=119147'}",[],93.0
119139,12,ELECTRA-Large 335M (fine-tuned on DiscoSense and HellaSwag),ELECTRA-Large 335M ,fine-tuned on DiscoSense and HellaSwag,2022-10-22,{'Accuracy': '91.5'},{'Accuracy': 91.5},False,"{'id': 1098872, 'title': 'DiscoSense: Commonsense Reasoning with Discourse Connectives', 'url': '/paper/discosense-commonsense-reasoning-with', 'published': '2022-10-22T00:00:00.000000', 'code': True, 'review_url': '/paper/discosense-commonsense-reasoning-with/review/?hl=119139'}",[],91.5
119888,13,DBRX Instruct 132B (10-shot),DBRX Instruct 132B ,10-shot,2024-03-27,{'Accuracy': '89'},{'Accuracy': 89.0},False,"{'id': None, 'title': None, 'url': None, 'published': None, 'code': False, 'review_url': None}",[],89.0
106870,14,TheBloke/llama-2-70b-Guanaco-QLoRA-fp16 (10-shot),TheBloke/llama-2-70b-Guanaco-QLoRA-fp16 ,10-shot,,{'Accuracy': '88.3'},{'Accuracy': 88.3},False,"{'id': None, 'title': None, 'url': None, 'published': None, 'code': False, 'review_url': None}",[],88.3
119145,15,ALBERT-XXL 235M,ALBERT-XXL 235M,,2021-05-29,{'Accuracy': '88'},{'Accuracy': 88.0},False,"{'id': None, 'title': None, 'url': None, 'published': None, 'code': False, 'review_url': None}",[],88.0
102519,16,PaLM 2-L (1-shot),PaLM 2-L ,1-shot,2023-05-17,{'Accuracy': '87.4'},{'Accuracy': 87.4},False,"{'id': 1210556, 'title': 'PaLM 2 Technical Report', 'url': '/paper/palm-2-technical-report-1', 'published': '2023-05-17T00:00:00.000000', 'code': True, 'review_url': '/paper/palm-2-technical-report-1/review/?hl=102519'}",[],87.4
119140,17,ELECTRA-Large 335M (fine-tuned on HellaSwag),ELECTRA-Large 335M ,fine-tuned on HellaSwag,2022-10-22,{'Accuracy': '86.9'},{'Accuracy': 86.9},False,"{'id': 1098872, 'title': 'DiscoSense: Commonsense Reasoning with Discourse Connectives', 'url': '/paper/discosense-commonsense-reasoning-with', 'published': '2022-10-22T00:00:00.000000', 'code': True, 'review_url': '/paper/discosense-commonsense-reasoning-with/review/?hl=119140'}",[],86.9
102518,18,PaLM 2-M (1-shot),PaLM 2-M ,1-shot,2023-05-17,{'Accuracy': '86.7'},{'Accuracy': 86.7},False,"{'id': 1210556, 'title': 'PaLM 2 Technical Report', 'url': '/paper/palm-2-technical-report-1', 'published': '2023-05-17T00:00:00.000000', 'code': True, 'review_url': '/paper/palm-2-technical-report-1/review/?hl=102518'}",[],86.7
35865,19,MUPPET Roberta Large,MUPPET Roberta Large,,2021-01-26,{'Accuracy': '86.4'},{'Accuracy': 86.4},False,"{'id': 739728, 'title': 'Muppet: Massive Multi-task Representations with Pre-Finetuning', 'url': '/paper/muppet-massive-multi-task-representations', 'published': '2021-01-26T00:00:00.000000', 'code': True, 'review_url': '/paper/muppet-massive-multi-task-representations/review/?hl=35865'}",[],86.4
105777,20,LLaMA 65B + CFG (0-shot),LLaMA 65B + CFG ,0-shot,2023-06-30,{'Accuracy': '86.3'},{'Accuracy': 86.3},False,"{'id': 1238561, 'title': 'Stay on topic with Classifier-Free Guidance', 'url': '/paper/stay-on-topic-with-classifier-free-guidance', 'published': '2023-06-30T00:00:00.000000', 'code': False, 'review_url': '/paper/stay-on-topic-with-classifier-free-guidance/review/?hl=105777'}",[],86.3
114908,21,Falcon-180B (0-shot),Falcon-180B ,0-shot,2023-11-28,{'Accuracy': '85.9'},{'Accuracy': 85.9},False,"{'id': 1329402, 'title': 'The Falcon Series of Open Language Models', 'url': '/paper/the-falcon-series-of-open-language-models', 'published': '2023-11-28T00:00:00.000000', 'code': False, 'review_url': '/paper/the-falcon-series-of-open-language-models/review/?hl=114908'}",[],85.9
102517,22,PaLM 2-S (1-shot),PaLM 2-S ,1-shot,2023-05-17,{'Accuracy': '85.6'},{'Accuracy': 85.6},False,"{'id': 1210556, 'title': 'PaLM 2 Technical Report', 'url': '/paper/palm-2-technical-report-1', 'published': '2023-05-17T00:00:00.000000', 'code': True, 'review_url': '/paper/palm-2-technical-report-1/review/?hl=102517'}",[],85.6
99241,23,GPT-3.5 (10-shot),GPT-3.5 ,10-shot,2023-03-15,{'Accuracy': '85.5'},{'Accuracy': 85.5},False,"{'id': 1174373, 'title': 'GPT-4 Technical Report', 'url': '/paper/gpt-4-technical-report-1', 'published': '2023-03-15T00:00:00.000000', 'code': True, 'review_url': '/paper/gpt-4-technical-report-1/review/?hl=99241'}",[],85.5
118422,24,RoBERTa-Large Ensemble,RoBERTa-Large Ensemble,,2019-07-26,{'Accuracy': '85.5'},{'Accuracy': 85.5},False,"{'id': 148282, 'title': 'RoBERTa: A Robustly Optimized BERT Pretraining Approach', 'url': '/paper/roberta-a-robustly-optimized-bert-pretraining', 'published': '2019-07-26T00:00:00.000000', 'code': True, 'review_url': '/paper/roberta-a-robustly-optimized-bert-pretraining/review/?hl=118422'}",[],85.5
105776,25,LLaMA 30B + CFG (0-shot),LLaMA 30B + CFG ,0-shot,2023-06-30,{'Accuracy': '85.3'},{'Accuracy': 85.3},False,"{'id': 1238561, 'title': 'Stay on topic with Classifier-Free Guidance', 'url': '/paper/stay-on-topic-with-classifier-free-guidance', 'published': '2023-06-30T00:00:00.000000', 'code': False, 'review_url': '/paper/stay-on-topic-with-classifier-free-guidance/review/?hl=105776'}",[],85.3
106311,26,LLaMA 2 70B (0-shot),LLaMA 2 70B ,0-shot,2023-07-18,{'Accuracy': '85.3'},{'Accuracy': 85.3},False,"{'id': 1248363, 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'url': '/paper/llama-2-open-foundation-and-fine-tuned-chat', 'published': '2023-07-18T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-2-open-foundation-and-fine-tuned-chat/review/?hl=106311'}",[],85.3
118421,27,HyKAS+CSKG,HyKAS+CSKG,,2019-10-30,{'Accuracy': '85.0'},{'Accuracy': 85.0},False,"{'id': 167490, 'title': 'Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering', 'url': '/paper/towards-generalizable-neuro-symbolic-systems', 'published': '2019-10-30T00:00:00.000000', 'code': False, 'review_url': '/paper/towards-generalizable-neuro-symbolic-systems/review/?hl=118421'}",[],85.0
97614,28,LLaMA 65B (0-shot),LLaMA 65B ,0-shot,2023-02-27,{'Accuracy': '84.2'},{'Accuracy': 84.2},False,"{'id': 1164350, 'title': 'LLaMA: Open and Efficient Foundation Language Models', 'url': '/paper/llama-open-and-efficient-foundation-language-1', 'published': '2023-02-27T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-open-and-efficient-foundation-language-1/review/?hl=97614'}",[],84.2
51447,29,PaLM-540B (Few-Shot),PaLM-540B ,Few-Shot,2022-04-05,{'Accuracy': '83.8'},{'Accuracy': 83.8},False,"{'id': 989558, 'title': 'PaLM: Scaling Language Modeling with Pathways', 'url': '/paper/palm-scaling-language-modeling-with-pathways-1', 'published': '2022-04-05T00:00:00.000000', 'code': True, 'review_url': None}","[{'id': 183, 'name': 'few-shot', 'color': '#a1df95'}]",83.8
51446,30,PaLM-540B (1-shot),PaLM-540B ,1-shot,2022-04-05,{'Accuracy': '83.6'},{'Accuracy': 83.6},False,"{'id': 989558, 'title': 'PaLM: Scaling Language Modeling with Pathways', 'url': '/paper/palm-scaling-language-modeling-with-pathways-1', 'published': '2022-04-05T00:00:00.000000', 'code': True, 'review_url': None}","[{'id': 214, 'name': 'one-shot', 'color': '#ea9e57'}]",83.6
119121,31,ExDeBERTa 567M,ExDeBERTa 567M,,2022-10-12,{'Accuracy': '83.6'},{'Accuracy': 83.6},False,"{'id': 1091660, 'title': 'Task Compass: Scaling Multi-task Pre-training with Task Prefix', 'url': '/paper/task-compass-scaling-multi-task-pre-training', 'published': '2022-10-12T00:00:00.000000', 'code': True, 'review_url': '/paper/task-compass-scaling-multi-task-pre-training/review/?hl=119121'}",[],83.6
51445,32,PaLM-540B (0-shot),PaLM-540B ,0-shot,2022-04-05,{'Accuracy': '83.4'},{'Accuracy': 83.4},False,"{'id': 989558, 'title': 'PaLM: Scaling Language Modeling with Pathways', 'url': '/paper/palm-scaling-language-modeling-with-pathways-1', 'published': '2022-04-05T00:00:00.000000', 'code': True, 'review_url': None}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",83.4
106310,33,LLaMA 2 34B (0-shot),LLaMA 2 34B ,0-shot,2023-07-18,{'Accuracy': '83.3'},{'Accuracy': 83.3},False,"{'id': 1248363, 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'url': '/paper/llama-2-open-foundation-and-fine-tuned-chat', 'published': '2023-07-18T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-2-open-foundation-and-fine-tuned-chat/review/?hl=106310'}",[],83.3
119097,34,Camelidae-8×34B (10-shot),Camelidae-8×34B ,10-shot,2024-01-05,{'Accuracy': '83.2'},{'Accuracy': 83.2},False,"{'id': 1355944, 'title': 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks', 'url': '/paper/parameter-efficient-sparsity-crafting-from', 'published': '2024-01-05T00:00:00.000000', 'code': True, 'review_url': '/paper/parameter-efficient-sparsity-crafting-from/review/?hl=119097'}",[],83.2
97613,35,LLaMA 33B (0-shot),LLaMA 33B ,0-shot,2023-02-27,{'Accuracy': '82.8'},{'Accuracy': 82.8},False,"{'id': 1164350, 'title': 'LLaMA: Open and Efficient Foundation Language Models', 'url': '/paper/llama-open-and-efficient-foundation-language-1', 'published': '2023-02-27T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-open-and-efficient-foundation-language-1/review/?hl=97613'}",[],82.8
114907,36,Falcon-40B (0-shot),Falcon-40B ,0-shot,2023-11-28,{'Accuracy': '82.7'},{'Accuracy': 82.7},False,"{'id': 1329402, 'title': 'The Falcon Series of Open Language Models', 'url': '/paper/the-falcon-series-of-open-language-models', 'published': '2023-11-28T00:00:00.000000', 'code': False, 'review_url': '/paper/the-falcon-series-of-open-language-models/review/?hl=114907'}",[],82.7
51451,37,Megatron-Turing NLG 530B (Few-Shot),Megatron-Turing NLG 530B ,Few-Shot,2022-01-28,{'Accuracy': '82.4'},{'Accuracy': 82.4},False,"{'id': 952554, 'title': 'Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model', 'url': '/paper/using-deepspeed-and-megatron-to-train', 'published': '2022-01-28T00:00:00.000000', 'code': True, 'review_url': '/paper/using-deepspeed-and-megatron-to-train/review/?hl=51451'}","[{'id': 183, 'name': 'few-shot', 'color': '#a1df95'}]",82.4
119111,38,Qwen2idae-16x14B (10-shot),Qwen2idae-16x14B ,10-shot,2024-03-12,{'Accuracy': '82.3'},{'Accuracy': 82.3},False,"{'id': 1355944, 'title': 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks', 'url': '/paper/parameter-efficient-sparsity-crafting-from', 'published': '2024-01-05T00:00:00.000000', 'code': True, 'review_url': '/paper/parameter-efficient-sparsity-crafting-from/review/?hl=119111'}",[],82.3
105778,39,LLaMA 13B + CFG (0-shot),LLaMA 13B + CFG ,0-shot,2023-06-30,{'Accuracy': '82.1'},{'Accuracy': 82.1},False,"{'id': 1238561, 'title': 'Stay on topic with Classifier-Free Guidance', 'url': '/paper/stay-on-topic-with-classifier-free-guidance', 'published': '2023-06-30T00:00:00.000000', 'code': False, 'review_url': '/paper/stay-on-topic-with-classifier-free-guidance/review/?hl=105778'}",[],82.1
118420,40,RoBERTa-Large 355M,RoBERTa-Large 355M,,2019-07-26,{'Accuracy': '81.7'},{'Accuracy': 81.7},False,"{'id': 148282, 'title': 'RoBERTa: A Robustly Optimized BERT Pretraining Approach', 'url': '/paper/roberta-a-robustly-optimized-bert-pretraining', 'published': '2019-07-26T00:00:00.000000', 'code': True, 'review_url': '/paper/roberta-a-robustly-optimized-bert-pretraining/review/?hl=118420'}",[],81.7
118936,41,Mistral 7B (0-shot),Mistral 7B ,0-shot,2023-10-10,{'Accuracy': '81.3'},{'Accuracy': 81.3},False,"{'id': 1297015, 'title': 'Mistral 7B', 'url': '/paper/mistral-7b', 'published': '2023-10-10T00:00:00.000000', 'code': True, 'review_url': '/paper/mistral-7b/review/?hl=118936'}",[],81.3
51449,42,Chinchilla 70B (0-shot),Chinchilla 70B ,0-shot,2022-03-29,{'Accuracy': '80.8'},{'Accuracy': 80.8},False,"{'id': 985465, 'title': 'Training Compute-Optimal Large Language Models', 'url': '/paper/training-compute-optimal-large-language', 'published': '2022-03-29T00:00:00.000000', 'code': True, 'review_url': None}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",80.8
122198,43,LLaMA 2 13B (0-shot),LLaMA 2 13B ,0-shot,2023-07-18,{'Accuracy': '80.7'},{'Accuracy': 80.7},False,"{'id': 1248363, 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'url': '/paper/llama-2-open-foundation-and-fine-tuned-chat', 'published': '2023-07-18T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-2-open-foundation-and-fine-tuned-chat/review/?hl=122198'}",[],80.7
51450,44,Megatron-Turing NLG 530B (1-shot),Megatron-Turing NLG 530B ,1-shot,2022-01-28,{'Accuracy': '80.2'},{'Accuracy': 80.2},False,"{'id': 952554, 'title': 'Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model', 'url': '/paper/using-deepspeed-and-megatron-to-train', 'published': '2022-01-28T00:00:00.000000', 'code': True, 'review_url': '/paper/using-deepspeed-and-megatron-to-train/review/?hl=51450'}","[{'id': 214, 'name': 'one-shot', 'color': '#ea9e57'}]",80.2
117940,45,"GPT-3 175B (few-shot, k=32)",GPT-3 175B ,"few-shot, k=32",2020-05-28,{'Accuracy': '79.3'},{'Accuracy': 79.3},False,"{'id': 198147, 'title': 'Language Models are Few-Shot Learners', 'url': '/paper/language-models-are-few-shot-learners', 'published': '2020-05-28T00:00:00.000000', 'code': True, 'review_url': '/paper/language-models-are-few-shot-learners/review/?hl=117940'}",[],79.3
66317,46,Gopher 280B (0-shot),Gopher 280B ,0-shot,2021-12-08,{'Accuracy': '79.2'},{'Accuracy': 79.2},False,"{'id': 942590, 'title': 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher', 'url': '/paper/scaling-language-models-methods-analysis-1', 'published': '2021-12-08T00:00:00.000000', 'code': True, 'review_url': None}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",79.2
97612,47,LLaMA 13B (0-shot),LLaMA 13B ,0-shot,2023-02-27,{'Accuracy': '79.2'},{'Accuracy': 79.2},False,"{'id': 1164350, 'title': 'LLaMA: Open and Efficient Foundation Language Models', 'url': '/paper/llama-open-and-efficient-foundation-language-1', 'published': '2023-02-27T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-open-and-efficient-foundation-language-1/review/?hl=97612'}",[],79.2
66316,48,GPT-3 (0-shot),GPT-3 ,0-shot,2020-05-28,{'Accuracy': '78.9'},{'Accuracy': 78.9},False,"{'id': 198147, 'title': 'Language Models are Few-Shot Learners', 'url': '/paper/language-models-are-few-shot-learners', 'published': '2020-05-28T00:00:00.000000', 'code': True, 'review_url': '/paper/language-models-are-few-shot-learners/review/?hl=66316'}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",78.9
106308,49,LLaMA 2 7B (0-shot),LLaMA 2 7B ,0-shot,2023-07-18,{'Accuracy': '77.2'},{'Accuracy': 77.2},False,"{'id': 1248363, 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'url': '/paper/llama-2-open-foundation-and-fine-tuned-chat', 'published': '2023-07-18T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-2-open-foundation-and-fine-tuned-chat/review/?hl=106308'}",[],77.2
114906,50,Falcon-7B (0-shot),Falcon-7B ,0-shot,2023-11-28,{'Accuracy': '76.3'},{'Accuracy': 76.3},False,"{'id': 1329402, 'title': 'The Falcon Series of Open Language Models', 'url': '/paper/the-falcon-series-of-open-language-models', 'published': '2023-11-28T00:00:00.000000', 'code': False, 'review_url': '/paper/the-falcon-series-of-open-language-models/review/?hl=114906'}",[],76.3
97611,51,LLaMA 7B (0-shot),LLaMA 7B ,0-shot,2023-02-27,{'Accuracy': '76.1'},{'Accuracy': 76.1},False,"{'id': 1164350, 'title': 'LLaMA: Open and Efficient Foundation Language Models', 'url': '/paper/llama-open-and-efficient-foundation-language-1', 'published': '2023-02-27T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-open-and-efficient-foundation-language-1/review/?hl=97611'}",[],76.1
100845,52,BlooombergGPT 50B (1-shot),BlooombergGPT 50B ,1-shot,2023-03-30,{'Accuracy': '73.9'},{'Accuracy': 73.9},False,"{'id': 1183339, 'title': 'BloombergGPT: A Large Language Model for Finance', 'url': '/paper/bloomberggpt-a-large-language-model-for', 'published': '2023-03-30T00:00:00.000000', 'code': False, 'review_url': '/paper/bloomberggpt-a-large-language-model-for/review/?hl=100845'}",[],73.9
100847,53,OPT 66B (1-shot),OPT 66B ,1-shot,2023-03-30,{'Accuracy': '73.5'},{'Accuracy': 73.5},False,"{'id': 1183339, 'title': 'BloombergGPT: A Large Language Model for Finance', 'url': '/paper/bloomberggpt-a-large-language-model-for', 'published': '2023-03-30T00:00:00.000000', 'code': False, 'review_url': '/paper/bloomberggpt-a-large-language-model-for/review/?hl=100847'}",[],73.5
100848,54,BLOOM 176B (1-shot),BLOOM 176B ,1-shot,2023-03-30,{'Accuracy': '73.2'},{'Accuracy': 73.2},False,"{'id': 1183339, 'title': 'BloombergGPT: A Large Language Model for Finance', 'url': '/paper/bloomberggpt-a-large-language-model-for', 'published': '2023-03-30T00:00:00.000000', 'code': False, 'review_url': '/paper/bloomberggpt-a-large-language-model-for/review/?hl=100848'}",[],73.2
112368,55,Sheared-LLaMA-2.7B (50B),Sheared-LLaMA-2.7B ,50B,2023-10-10,{'Accuracy': '70.8'},{'Accuracy': 70.8},False,"{'id': 1297030, 'title': 'Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning', 'url': '/paper/sheared-llama-accelerating-language-model-pre', 'published': '2023-10-10T00:00:00.000000', 'code': True, 'review_url': '/paper/sheared-llama-accelerating-language-model-pre/review/?hl=112368'}",[],70.8
100846,56,GPT-NeoX 20B (1-shot),GPT-NeoX 20B ,1-shot,2023-03-30,{'Accuracy': '68.4'},{'Accuracy': 68.4},False,"{'id': 1183339, 'title': 'BloombergGPT: A Large Language Model for Finance', 'url': '/paper/bloomberggpt-a-large-language-model-for', 'published': '2023-03-30T00:00:00.000000', 'code': False, 'review_url': '/paper/bloomberggpt-a-large-language-model-for/review/?hl=100846'}",[],68.4
112367,57,Open-LLaMA-3B-v2,Open-LLaMA-3B-v2,,2023-10-10,{'Accuracy': '67.6'},{'Accuracy': 67.6},False,"{'id': 1297030, 'title': 'Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning', 'url': '/paper/sheared-llama-accelerating-language-model-pre', 'published': '2023-10-10T00:00:00.000000', 'code': True, 'review_url': '/paper/sheared-llama-accelerating-language-model-pre/review/?hl=112367'}",[],67.6
113982,58,Mamba-2.8B,Mamba-2.8B,,2023-12-01,{'Accuracy': '66.1'},{'Accuracy': 66.1},False,"{'id': 1334747, 'title': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'url': '/paper/mamba-linear-time-sequence-modeling-with', 'published': '2023-12-01T00:00:00.000000', 'code': True, 'review_url': None}",[],66.1
112366,59,Sheared-LLaMA-1.3B (50B),Sheared-LLaMA-1.3B ,50B,2023-10-10,{'Accuracy': '60.7'},{'Accuracy': 60.7},False,"{'id': 1297030, 'title': 'Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning', 'url': '/paper/sheared-llama-accelerating-language-model-pre', 'published': '2023-10-10T00:00:00.000000', 'code': True, 'review_url': '/paper/sheared-llama-accelerating-language-model-pre/review/?hl=112366'}",[],60.7
118115,60,FLAN 137B (3-shot),FLAN 137B ,3-shot,2021-09-03,{'Accuracy': '59.2'},{'Accuracy': 59.2},False,"{'id': 861409, 'title': 'Finetuned Language Models Are Zero-Shot Learners', 'url': '/paper/finetuned-language-models-are-zero-shot', 'published': '2021-09-03T00:00:00.000000', 'code': True, 'review_url': '/paper/finetuned-language-models-are-zero-shot/review/?hl=118115'}",[],59.2
113983,61,Mamba-1.4B,Mamba-1.4B,,2023-12-01,{'Accuracy': '59.1'},{'Accuracy': 59.1},False,"{'id': 1334747, 'title': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'url': '/paper/mamba-linear-time-sequence-modeling-with', 'published': '2023-12-01T00:00:00.000000', 'code': True, 'review_url': None}",[],59.1
39037,62,FLAN 137B (0-shot),FLAN 137B ,0-shot,2021-09-03,{'Accuracy': '56.7'},{'Accuracy': 56.7},False,"{'id': 861409, 'title': 'Finetuned Language Models Are Zero-Shot Learners', 'url': '/paper/finetuned-language-models-are-zero-shot', 'published': '2021-09-03T00:00:00.000000', 'code': True, 'review_url': '/paper/finetuned-language-models-are-zero-shot/review/?hl=39037'}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",56.7
117972,63,sMLP – deterministic 9.4B (0-shot),sMLP – deterministic 9.4B ,0-shot,2022-03-14,{'Accuracy': '54.5'},{'Accuracy': 54.5},False,"{'id': 976184, 'title': 'Efficient Language Modeling with Sparse all-MLP', 'url': '/paper/efficient-language-modeling-with-sparse-all', 'published': '2022-03-14T00:00:00.000000', 'code': False, 'review_url': '/paper/efficient-language-modeling-with-sparse-all/review/?hl=117972'}",[],54.5
117962,64,Switch Transformer 9B,Switch Transformer 9B,,2022-03-14,{'Accuracy': '52.5'},{'Accuracy': 52.5},False,"{'id': 976184, 'title': 'Efficient Language Modeling with Sparse all-MLP', 'url': '/paper/efficient-language-modeling-with-sparse-all', 'published': '2022-03-14T00:00:00.000000', 'code': False, 'review_url': '/paper/efficient-language-modeling-with-sparse-all/review/?hl=117962'}",[],52.5
118007,65,GPT-3 Large 760M (0-shot),GPT-3 Large 760M ,0-shot,2020-05-28,{'Accuracy': '51.0'},{'Accuracy': 51.0},False,"{'id': 198147, 'title': 'Language Models are Few-Shot Learners', 'url': '/paper/language-models-are-few-shot-learners', 'published': '2020-05-28T00:00:00.000000', 'code': True, 'review_url': '/paper/language-models-are-few-shot-learners/review/?hl=118007'}",[],51.0
117860,66,GPT-2-XL 1.5B,GPT-2-XL 1.5B,,2023-04-27,{'Accuracy': '50.9'},{'Accuracy': 50.9},False,"{'id': 1198818, 'title': 'LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions', 'url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models', 'published': '2023-04-27T00:00:00.000000', 'code': True, 'review_url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models/review/?hl=117860'}",[],50.9
114601,67,OPT-6.7B,OPT-6.7B,,2023-12-12,{'Accuracy': '50.3'},{'Accuracy': 50.3},False,"{'id': 1346428, 'title': 'LLM in a flash: Efficient Large Language Model Inference with Limited Memory', 'url': '/paper/llm-in-a-flash-efficient-large-language-model', 'published': '2023-12-12T00:00:00.000000', 'code': False, 'review_url': None}",[],50.3
114600,68,LLM in a Flash (OPT-6.7B with Predictor),LLM in a Flash ,OPT-6.7B with Predictor,2023-12-12,{'Accuracy': '49.8'},{'Accuracy': 49.8},False,"{'id': 1346428, 'title': 'LLM in a flash: Efficient Large Language Model Inference with Limited Memory', 'url': '/paper/llm-in-a-flash-efficient-large-language-model', 'published': '2023-12-12T00:00:00.000000', 'code': False, 'review_url': None}",[],49.8
118239,69,FLAN-T5-Large 783M,FLAN-T5-Large 783M,,2023-04-27,{'Accuracy': '48.7'},{'Accuracy': 48.7},False,"{'id': 1198818, 'title': 'LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions', 'url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models', 'published': '2023-04-27T00:00:00.000000', 'code': True, 'review_url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models/review/?hl=118239'}",[],48.7
117861,70,LaMini-GPT 1.5B,LaMini-GPT 1.5B,,2023-04-27,{'Accuracy': '48.3'},{'Accuracy': 48.3},False,"{'id': 1198818, 'title': 'LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions', 'url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models', 'published': '2023-04-27T00:00:00.000000', 'code': True, 'review_url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models/review/?hl=117861'}",[],48.3
119120,71,BERT-Large 340M,BERT-Large 340M,,2019-05-19,{'Accuracy': '47.3'},{'Accuracy': 47.3},False,"{'id': 115101, 'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?', 'url': '/paper/hellaswag-can-a-machine-really-finish-your', 'published': '2019-05-19T00:00:00.000000', 'code': True, 'review_url': '/paper/hellaswag-can-a-machine-really-finish-your/review/?hl=119120'}",[],47.3
117859,72,LaMini-F-T5 783M,LaMini-F-T5 783M,,2023-04-27,{'Accuracy': '43.7'},{'Accuracy': 43.7},False,"{'id': 1198818, 'title': 'LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions', 'url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models', 'published': '2023-04-27T00:00:00.000000', 'code': True, 'review_url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models/review/?hl=117859'}",[],43.7
119118,73,GPT-1 117M,GPT-1 117M,,2019-05-19,{'Accuracy': '41.7'},{'Accuracy': 41.7},False,"{'id': 115101, 'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?', 'url': '/paper/hellaswag-can-a-machine-really-finish-your', 'published': '2019-05-19T00:00:00.000000', 'code': True, 'review_url': '/paper/hellaswag-can-a-machine-really-finish-your/review/?hl=119118'}",[],41.7
118709,74,Flipped-3B,Flipped-3B,,2022-10-06,{'Accuracy': '41.6'},{'Accuracy': 41.6},False,"{'id': 1087763, 'title': 'Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners', 'url': '/paper/guess-the-instruction-making-language-models', 'published': '2022-10-06T00:00:00.000000', 'code': True, 'review_url': '/paper/guess-the-instruction-making-language-models/review/?hl=118709'}",[],41.6
118692,75,T0-3B (CoT fine-tuned),T0-3B ,CoT fine-tuned,2023-05-23,{'Accuracy': '41.1'},{'Accuracy': 41.1},False,"{'id': 1214294, 'title': 'The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning', 'url': '/paper/the-cot-collection-improving-zero-shot-and', 'published': '2023-05-23T00:00:00.000000', 'code': True, 'review_url': '/paper/the-cot-collection-improving-zero-shot-and/review/?hl=118692'}",[],41.1
117857,76,LaMini-T5 738M,LaMini-T5 738M,,2023-04-27,{'Accuracy': '40.6'},{'Accuracy': 40.6},False,"{'id': 1198818, 'title': 'LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions', 'url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models', 'published': '2023-04-27T00:00:00.000000', 'code': True, 'review_url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models/review/?hl=117857'}",[],40.6
119119,77,BERT-Base 110M,BERT-Base 110M,,2019-05-19,{'Accuracy': '40.5'},{'Accuracy': 40.5},False,"{'id': 115101, 'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?', 'url': '/paper/hellaswag-can-a-machine-really-finish-your', 'published': '2019-05-19T00:00:00.000000', 'code': True, 'review_url': '/paper/hellaswag-can-a-machine-really-finish-your/review/?hl=119119'}",[],40.5
118238,78,T5-Large 738M,T5-Large 738M,,2023-04-27,{'Accuracy': '38.9'},{'Accuracy': 38.9},False,"{'id': 1198818, 'title': 'LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions', 'url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models', 'published': '2023-04-27T00:00:00.000000', 'code': True, 'review_url': '/paper/lamini-lm-a-diverse-herd-of-distilled-models/review/?hl=118238'}",[],38.9
117960,79,Gshard 9B,Gshard 9B,,2022-03-14,{'Accuracy': '38'},{'Accuracy': 38.0},False,"{'id': 976184, 'title': 'Efficient Language Modeling with Sparse all-MLP', 'url': '/paper/efficient-language-modeling-with-sparse-all', 'published': '2022-03-14T00:00:00.000000', 'code': False, 'review_url': '/paper/efficient-language-modeling-with-sparse-all/review/?hl=117960'}",[],38.0
119116,80,LSTM + BERT-Base,LSTM + BERT-Base,,2019-05-19,{'Accuracy': '36.2'},{'Accuracy': 36.2},False,"{'id': 115101, 'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?', 'url': '/paper/hellaswag-can-a-machine-really-finish-your', 'published': '2019-05-19T00:00:00.000000', 'code': True, 'review_url': '/paper/hellaswag-can-a-machine-really-finish-your/review/?hl=119116'}",[],36.2
118718,81,RoE-3B,RoE-3B,,2023-02-07,{'Accuracy': '34.6'},{'Accuracy': 34.6},False,"{'id': 1154202, 'title': 'Exploring the Benefits of Training Expert Language Models over Instruction Tuning', 'url': '/paper/exploring-the-benefits-of-training-expert', 'published': '2023-02-07T00:00:00.000000', 'code': True, 'review_url': '/paper/exploring-the-benefits-of-training-expert/review/?hl=118718'}",[],34.6
119117,82,ESIM + ElMo,ESIM + ElMo,,2019-05-19,{'Accuracy': '33.3'},{'Accuracy': 33.3},False,"{'id': 115101, 'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?', 'url': '/paper/hellaswag-can-a-machine-really-finish-your', 'published': '2019-05-19T00:00:00.000000', 'code': True, 'review_url': '/paper/hellaswag-can-a-machine-really-finish-your/review/?hl=119117'}",[],33.3
118004,83,HASH Layers 10B (0-shot),HASH Layers 10B ,0-shot,2022-03-14,{'Accuracy': '33'},{'Accuracy': 33.0},False,"{'id': 976184, 'title': 'Efficient Language Modeling with Sparse all-MLP', 'url': '/paper/efficient-language-modeling-with-sparse-all', 'published': '2022-03-14T00:00:00.000000', 'code': False, 'review_url': '/paper/efficient-language-modeling-with-sparse-all/review/?hl=118004'}",[],33.0
119114,84,LSTM + GloVe,LSTM + GloVe,,2019-05-19,{'Accuracy': '31.7'},{'Accuracy': 31.7},False,"{'id': 115101, 'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?', 'url': '/paper/hellaswag-can-a-machine-really-finish-your', 'published': '2019-05-19T00:00:00.000000', 'code': True, 'review_url': '/paper/hellaswag-can-a-machine-really-finish-your/review/?hl=119114'}",[],31.7
119113,85,fastText,fastText,,2019-05-19,{'Accuracy': '31.6'},{'Accuracy': 31.6},False,"{'id': 115101, 'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?', 'url': '/paper/hellaswag-can-a-machine-really-finish-your', 'published': '2019-05-19T00:00:00.000000', 'code': True, 'review_url': '/paper/hellaswag-can-a-machine-really-finish-your/review/?hl=119113'}",[],31.6
119115,86,LSTM + ElMo,LSTM + ElMo,,2019-05-19,{'Accuracy': '31.4'},{'Accuracy': 31.4},False,"{'id': 115101, 'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?', 'url': '/paper/hellaswag-can-a-machine-really-finish-your', 'published': '2019-05-19T00:00:00.000000', 'code': True, 'review_url': '/paper/hellaswag-can-a-machine-really-finish-your/review/?hl=119115'}",[],31.4
117998,87,Base Layers 10B (0-shot),Base Layers 10B ,0-shot,2022-03-14,{'Accuracy': '30.2'},{'Accuracy': 30.2},False,"{'id': 976184, 'title': 'Efficient Language Modeling with Sparse all-MLP', 'url': '/paper/efficient-language-modeling-with-sparse-all', 'published': '2022-03-14T00:00:00.000000', 'code': False, 'review_url': '/paper/efficient-language-modeling-with-sparse-all/review/?hl=117998'}",[],30.2
118701,88,KiC-770M,KiC-770M,,2022-10-28,{'Accuracy': '29.6'},{'Accuracy': 29.6},False,"{'id': 1103018, 'title': 'Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models', 'url': '/paper/knowledge-in-context-towards-knowledgeable', 'published': '2022-10-28T00:00:00.000000', 'code': False, 'review_url': '/paper/knowledge-in-context-towards-knowledgeable/review/?hl=118701'}",[],29.6
119112,89,Random chance baseline,Random chance baseline,,2019-05-19,{'Accuracy': '25'},{'Accuracy': 25.0},False,"{'id': 115101, 'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?', 'url': '/paper/hellaswag-can-a-machine-really-finish-your', 'published': '2019-05-19T00:00:00.000000', 'code': True, 'review_url': '/paper/hellaswag-can-a-machine-really-finish-your/review/?hl=119112'}",[],25.0
