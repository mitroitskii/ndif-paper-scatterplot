paper,year,link
"S. Abnar and W. Zuidema. Quantifying attention flow in transformers. In D. Jurafsky, J. Chai, N. Schluter, and
J. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
pp. 4190–4197, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl
-main.385. URL https://aclanthology.org/2020.acl-main.385.",2020,https://aclanthology.org/2020.acl-main.385.
"R. Achtibat, S. M. V. Hatefi, M. Dreyer, A. Jain, T. Wiegand, S. Lapuschkin, and W. Samek. AttnLRP:
Attention-aware layer-wise relevance propagation for transformers, 2024.",2024,
"J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and B. Kim. Sanity checks for saliency maps. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, pp.
9525–9536, Red Hook, NY, USA, 2018. Curran Associates Inc.",2018,
"J. Adebayo, M. Muelly, I. Liccardi, and B. Kim. Debugging tests for model explanations. In Proceedings of the
34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY, USA,
2020. Curran Associates Inc. ISBN 9781713829546.",2020,
"J. Adebayo, M. Muelly, H. Abelson, and B. Kim. Post hoc explanations may be ineffective for detecting
unknown spurious correlation. In International Conference on Learning Representations, 2022. URL http
s://openreview.net/forum?id=xNOVfCCvDpM.",2022,"http
s://openreview.net/forum?id=xNOVfCCvDpM."
"C. Agarwal, S. H. Tanneru, and H. Lakkaraju. Faithfulness vs. plausibility: On the (un)reliability of explanations from large language models. ArXiv, abs/2402.04614, 2024. URL https://api.semanticscholar.
org/CorpusID:267523276.",2024,"https://api.semanticscholar.
org/CorpusID:267523276."
"A. Ahmadian, S. Dash, H. Chen, B. Venkitesh, Z. S. Gou, P. Blunsom, A. Üstün, and S. Hooker. Intriguing
properties of quantization at scale. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URL https://openreview.net/forum?id=IYe8j7Gy8f.",2023,https://openreview.net/forum?id=IYe8j7Gy8f.
"E. Akyurek, T. Bolukbasi, F. Liu, B. Xiong, I. Tenney, J. Andreas, and K. Guu. Towards tracing knowledge
in language models back to the training data. In Y. Goldberg, Z. Kozareva, and Y. Zhang (eds.), Findings
of the Association for Computational Linguistics: EMNLP 2022, pp. 2429–2446, Abu Dhabi, United Arab
Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-e
mnlp.180. URL https://aclanthology.org/2022.findings-emnlp.180.",2022,https://aclanthology.org/2022.findings-emnlp.180.
"E. Akyürek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context learning?
investigations with linear models. In The Eleventh International Conference on Learning Representations,
2023. URL https://openreview.net/forum?id=0g0X4H8yN4I.",2023,https://openreview.net/forum?id=0g0X4H8yN4I.
"E. Akyürek, B. Wang, Y. Kim, and J. Andreas. In-context language learning: Architectures and algorithms,
2024.",2024,
"G. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes. Arxiv, 2016. URL
https://arxiv.org/abs/1610.01644.",2016,https://arxiv.org/abs/1610.01644.
"J. Alammar. Ecco: An open source library for the explainability of transformer language models. In H. Ji,
J. C. Park, and R. Xia (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 249–257, Online, August 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.acl-demo.30. URL https://aclanthology.org/2021.acl-demo.30.",2021,https://aclanthology.org/2021.acl-demo.30.
"A. Ali, T. Schnake, O. Eberle, G. Montavon, K.-R. Müller, and L. Wolf. XAI for transformers: Better
explanations through conservative propagation. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,
G. Niu, and S. Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning,
volume 162 of Proceedings of Machine Learning Research, pp. 435–451. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/ali22a.html.",2022,https://proceedings.mlr.press/v162/ali22a.html.
"A. Ali, I. Zimerman, and L. Wolf. The hidden attention of mamba models, 2024.",2024,
"K. Amara, R. Sevastjanova, and M. El-Assady. Syntaxshap: Syntax-aware explainability method for text
generation. ArXiv, abs/2402.09259, 2024. URL https://api.semanticscholar.org/CorpusID:
267657673.",2024,"https://api.semanticscholar.org/CorpusID:
267657673."
"C. Anil, Y. Wu, A. J. Andreassen, A. Lewkowycz, V. Misra, V. V. Ramasesh, A. Slone, G. Gur-Ari, E. Dyer,
and B. Neyshabur. Exploring length generalization in large language models. In A. H. Oh, A. Agarwal,
D. Belgrave, and K. Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https:
//openreview.net/forum?id=zSkYVeX7bC4.",2022,"https:
//openreview.net/forum?id=zSkYVeX7bC4."
"Anonymous. The disagreement problem in explainable machine learning: A practitioner’s perspective. Submitted to Transactions on Machine Learning Research, 2024. URL https://openreview.net/forum?id=
jESY2WTZCe. Under review.",2024,"https://openreview.net/forum?id=
jESY2WTZCe. Under review."
"A. Arditi, O. Balcells, A. Syed, W. Gurnee, and N. Nanda. Refusal in llms is mediated by a single direction.
Alignment Forum, 2024. URL https://alignmentforum.org/posts/jGuXSZgv6qfdhMCuJ/refusal-i
n-llms-is-mediated-by-a-single-direction.",2024,"https://alignmentforum.org/posts/jGuXSZgv6qfdhMCuJ/refusal-i
n-llms-is-mediated-by-a-single-direction."
"A. Arora, D. Jurafsky, and C. Potts. Causalgym: Benchmarking causal interpretability methods on linguistic
tasks, 2024. URL https://arxiv.org/abs/2402.12560.",2024,https://arxiv.org/abs/2402.12560.
"S. Arora, Y. Li, Y. Liang, T. Ma, and A. Risteski. Linear algebraic structure of word senses, with applications
to polysemy. Transactions of the Association for Computational Linguistics, 6:483–495, 2018. doi: 10.116
2/tacl_a_00034. URL https://aclanthology.org/Q18-1034.",2018,https://aclanthology.org/Q18-1034.
"P. Atanasova, J. G. Simonsen, C. Lioma, and I. Augenstein. A diagnostic study of explainability techniques
for text classification. In B. Webber, T. Cohn, Y. He, and Y. Liu (eds.), Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pp. 3256–3274, Online, November 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.263. URL https:
//aclanthology.org/2020.emnlp-main.263.",2020,"https:
//aclanthology.org/2020.emnlp-main.263."
"P. Atanasova, O.-M. Camburu, C. Lioma, T. Lukasiewicz, J. G. Simonsen, and I. Augenstein. Faithfulness
tests for natural language explanations. In A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.), Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp.
283–294, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.a
cl-short.25. URL https://aclanthology.org/2023.acl-short.25.",2023,https://aclanthology.org/2023.acl-short.25.
"G. Attanasio, E. Pastor, C. Di Bonaventura, and D. Nozza. ferret: a framework for benchmarking explainers
on transformers. In D. Croce and L. Soldaini (eds.), Proceedings of the 17th Conference of the European
Chapter of the Association for Computational Linguistics: System Demonstrations, pp. 256–266, Dubrovnik,
Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-demo.29. URL
https://aclanthology.org/2023.eacl-demo.29.",2023,https://aclanthology.org/2023.eacl-demo.29.
"M. Avitan, R. Cotterell, Y. Goldberg, and S. Ravfogel. What changed? converting representational interventions
to natural language. Arxiv, 2024. URL https://arxiv.org/abs/2402.11355.",2024,https://arxiv.org/abs/2402.11355.
"A. Azaria and T. Mitchell. The internal state of an LLM knows when it’s lying. In H. Bouamor, J. Pino,
and K. Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 967–976,
Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-e
mnlp.68. URL https://aclanthology.org/2023.findings-emnlp.68.",2023,https://aclanthology.org/2023.findings-emnlp.68.
"J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. Arxiv, 2016. URL https://arxiv.org/abs/16
07.06450.",2016,"https://arxiv.org/abs/16
07.06450."
"S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek. On pixel-wise explanations for
non-linear classifier decisions by layer-wise relevance propagation. PLOS ONE, 10(7):1–46, 07 2015. doi:
10.1371/journal.pone.0130140. URL https://doi.org/10.1371/journal.pone.0130140.",2015,https://doi.org/10.1371/journal.pone.0130140.
"Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,
N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez,
T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training a helpful and harmless assistant with reinforcement
learning from human feedback. ArXiv, 2022.",2022,
"D. Balduzzi, M. Frean, L. Leary, J. P. Lewis, K. W.-D. Ma, and B. McWilliams. The shattered gradients
problem: If resnets are the answer, then what is the question? In D. Precup and Y. W. Teh (eds.), Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 342–350. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/balduz
zi17b.html.",2017,"https://proceedings.mlr.press/v70/balduz
zi17b.html."
"J. Bastings and K. Filippova. The elephant in the interpretability room: Why use attention as explanation
when we have saliency methods? In A. Alishahi, Y. Belinkov, G. Chrupała, D. Hupkes, Y. Pinter, and
H. Sajjad (eds.), Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural
Networks for NLP, pp. 149–155, Online, November 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.blackboxnlp-1.14. URL https://aclanthology.org/2020.blackboxnlp-1.14.",2020,https://aclanthology.org/2020.blackboxnlp-1.14.
"J. Bastings, S. Ebert, P. Zablotskaia, A. Sandholm, and K. Filippova. “will you find these shortcuts?” a
protocol for evaluating the faithfulness of input salience methods for text classification. In Y. Goldberg,
Z. Kozareva, and Y. Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, pp. 976–991, Abu Dhabi, United Arab Emirates, December 2022. Association for
Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.64. URL https://aclanthology.org/2
022.emnlp-main.64.",2022,"https://aclanthology.org/2
022.emnlp-main.64."
"A. Bau, Y. Belinkov, H. Sajjad, N. Durrani, F. Dalvi, and J. Glass. Identifying and controlling important
neurons in neural machine translation. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=H1z-PsR5KX.",2019,https://openreview.net/forum?id=H1z-PsR5KX.
"D. Bau, J.-Y. Zhu, H. Strobelt, A. Lapedriza, B. Zhou, and A. Torralba. Understanding the role of individual
units in a deep neural network. Proceedings of the National Academy of Sciences, 117(48):30071–30078,
2020. doi: 10.1073/pnas.1907375117. URL https://www.pnas.org/doi/abs/10.1073/pnas.19073751
17.",2020,"https://www.pnas.org/doi/abs/10.1073/pnas.19073751
17."
"D. Bau, B. C. Wallace, A. Guha, J. Bell, and C. Brodley. National deep inference facility for very large language
models (ndif). United States National Science Foundation, 2023.",2023,
"Y. Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):
207–219, March 2022. doi: 10.1162/coli_a_00422. URL https://aclanthology.org/2022.cl-1.7.",2022,https://aclanthology.org/2022.cl-1.7.
"Y. Belinkov and J. Glass. Analysis methods in neural language processing: A survey. Transactions of the
Association for Computational Linguistics, 7:49–72, 2019. doi: 10.1162/tacl_a_00254. URL https:
//aclanthology.org/Q19-1004.",2019,"https:
//aclanthology.org/Q19-1004."
"Y. Belinkov, N. Durrani, F. Dalvi, H. Sajjad, and J. Glass. What do neural machine translation models learn
about morphology? In R. Barzilay and M.-Y. Kan (eds.), Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 861–872, Vancouver, Canada, July
2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1080. URL https://aclantholo
gy.org/P17-1080.",2017,"https://aclantholo
gy.org/P17-1080."
"N. Belrose. Least-squares concept erasure with oracle concept labels. EleutherAI Blog, 2023. URL https:
//blog.eleuther.ai/oracle-leace/.",2023,"https:
//blog.eleuther.ai/oracle-leace/."
"N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney, S. Biderman, and J. Steinhardt.
Eliciting latent predictions from transformers with the tuned lens. Arxiv, 2023a. URL https://arxiv.or
g/abs/2303.08112.",,"https://arxiv.or
g/abs/2303.08112."
"N. Belrose, D. Schneider-Joseph, S. Ravfogel, R. Cotterell, E. Raff, and S. Biderman. LEACE: Perfect linear
concept erasure in closed form. In Thirty-seventh Conference on Neural Information Processing Systems,
2023b. URL https://openreview.net/forum?id=awIpKpwTwF.",,https://openreview.net/forum?id=awIpKpwTwF.
"Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. J. Mach. Learn.
Res., 3(null):1137–1155, mar 2003. ISSN 1532-4435.",2003,
"L. Bereska and E. Gavves. Mechanistic interpretability for ai safety – a review. ArXiv, 2024. URL https:
//arxiv.org/abs/2404.14082.",2024,"https:
//arxiv.org/abs/2404.14082."
"L. Berglund, M. Tong, M. Kaufmann, M. Balesni, A. C. Stickland, T. Korbak, and O. Evans. The reversal
curse: Llms trained on ""a is b"" fail to learn ""b is a"". ArXiv, abs/2309.12288, 2023. URL https://api.se
manticscholar.org/CorpusID:262083829.",2023,"https://api.se
manticscholar.org/CorpusID:262083829."
"A. Bibal, R. Cardon, D. Alfter, R. Wilkens, X. Wang, T. François, and P. Watrin. Is attention explanation? an
introduction to the debate. In S. Muresan, P. Nakov, and A. Villavicencio (eds.), Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3889–3900,
Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.269.
URL https://aclanthology.org/2022.acl-long.269.",2022,https://aclanthology.org/2022.acl-long.269.
"S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S.
Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. Van Der Wal. Pythia: a suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine
Learning, ICML’23. JMLR.org, 2023.",2023,
"A. Bietti, V. Cabannes, D. Bouchacourt, H. Jegou, and L. Bottou. Birth of a transformer: A memory viewpoint.
In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural
Information Processing Systems, volume 36, pp. 1560–1588. Curran Associates, Inc., 2023. URL https:
//proceedings.neurips.cc/paper_files/paper/2023/file/0561738a239a995c8cd2ef0e50cfa4f
d-Paper-Conference.pdf.",2023,"https:
//proceedings.neurips.cc/paper_files/paper/2023/file/0561738a239a995c8cd2ef0e50cfa4f
d-Paper-Conference.pdf."
"S. Bills, N. Cammarata, D. Mossing, H. Tillman, L. Gao, G. Goh, I. Sutskever, J. Leike, J. Wu, and W. Saunders.
Language models can explain neurons in language models. https://openaipublic.blob.core.windows
.net/neuron-explainer/paper/index.html, 2023.",2023,
"B. Bilodeau, N. Jaques, P. W. Koh, and B. Kim. Impossibility theorems for feature attribution. Proceedings
of the National Academy of Sciences, 121(2):e2304406120, 2024. doi: 10.1073/pnas.2304406120. URL
https://www.pnas.org/doi/abs/10.1073/pnas.2304406120.",2024,https://www.pnas.org/doi/abs/10.1073/pnas.2304406120.
"J. Bloom. Open source sparse autoencoders for all residual stream layers of GPT2 small. AI Alignment Forum,
2024. URL https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/open-source-sparse-a
utoencoders-for-all-residual-stream.",2024,"https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/open-source-sparse-a
utoencoders-for-all-residual-stream."
"J. Bloom and D. Channin. Saelens. GitHub repository, 2024. URL https://github.com/jbloomAus/SAELe
ns.",2024,"https://github.com/jbloomAus/SAELe
ns."
"J. Bloom and J. Lin. Understanding SAE features with the logit lens. AI Alignment Forum, 2024. URL
https://www.alignmentforum.org/posts/qykrYY6rXXM7EEs8Q/understanding-sae-features-wit
h-the-logit-lens.",2024,"https://www.alignmentforum.org/posts/qykrYY6rXXM7EEs8Q/understanding-sae-features-wit
h-the-logit-lens."
"T. Bolukbasi, A. Pearce, A. Yuan, A. Coenen, E. Reif, F. Viégas, and M. Wattenberg. An interpretability illusion
for bert, 2021.",2021,
"Y. Bondarenko, M. Nagel, and T. Blankevoort. Quantizable transformers: Removing outliers by helping attention heads do nothing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL
https://openreview.net/forum?id=sbusw6LD41.",2023,https://openreview.net/forum?id=sbusw6LD41.
"T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison,
A. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer, T. Maxwell, N. Joseph, Z. Hatfield-Dodds, A. Tamkin,
K. Nguyen, B. McLean, J. E. Burke, T. Hume, S. Carter, T. Henighan, and C. Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. URL
https://transformer-circuits.pub/2023/monosemantic-features/index.html.",2023,https://transformer-circuits.pub/2023/monosemantic-features/index.html.
"S. Brody, U. Alon, and E. Yahav. On the expressivity role of LayerNorm in transformers’ attention. In
A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 14211–14221, Toronto, Canada, July 2023. Association for Computational Linguistics.
doi: 10.18653/v1/2023.findings-acl.895. URL https://aclanthology.org/2023.findings-acl.895.",2023,https://aclanthology.org/2023.findings-acl.895.
"D. Brown, N. Vyas, and Y. Bansal. On privileged and convergent bases in neural network representations,
2023.",2023,
"T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,
C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle,
M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/pap
er_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.",1901,"https://proceedings.neurips.cc/pap
er_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf."
"M.-E. Brunet, C. Alkalay-Houlihan, A. Anderson, and R. Zemel. Understanding the origins of bias in word
embeddings. In K. Chaudhuri and R. Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 803–811. PMLR, 09–
15 Jun 2019. URL https://proceedings.mlr.press/v97/brunet19a.html.",2019,https://proceedings.mlr.press/v97/brunet19a.html.
"G. Brunner, Y. Liu, D. Pascual, O. Richter, M. Ciaramita, and R. Wattenhofer. On identifiability in transformers.
In International Conference on Learning Representations, 2020. URL https://openreview.net/forum
?id=BJg1f6EFDB.",2020,"https://openreview.net/forum
?id=BJg1f6EFDB."
"C. Burns, H. Ye, D. Klein, and J. Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=ETKGuby0hcs.",2023,https://openreview.net/forum?id=ETKGuby0hcs.
"N. Cammarata, S. Carter, G. Goh, C. Olah, M. Petrov, L. Schubert, C. Voss, B. Egan, and S. K. Lim. Thread:
Circuits. Distill, 2020. doi: 10.23915/distill.00024. URL https://distill.pub/2020/circuits.",2020,https://distill.pub/2020/circuits.
"N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. URL https://arxiv.org/abs/2402.0
9221.",2024,"https://arxiv.org/abs/2402.0
9221."
"S. Casper, C. Ezell, C. Siegmann, N. Kolt, T. L. Curtis, B. Bucknall, A. A. Haupt, K. Wei, J. Scheurer,
M. Hobbhahn, L. Sharkey, S. Krishna, M. von Hagen, S. Alberti, A. Chan, Q. Sun, M. Gerovitch, D. Bau,
M. Tegmark, D. Krueger, and D. Hadfield-Menell. Black-box access is insufficient for rigorous ai audits.
ArXiv, abs/2401.14446, 2024. URL https://api.semanticscholar.org/CorpusID:267301601.",2024,https://api.semanticscholar.org/CorpusID:267301601.
"S. CH-Wang, B. V. Durme, J. Eisner, and C. Kedzie. Do androids know they’re only dreaming of electric
sheep?, 2023. URL https://arxiv.org/abs/2312.17249v1.",2023,https://arxiv.org/abs/2312.17249v1.
"L. Chan, A. Garriga-Alonso, N. Goldwosky-Dill, R. Greenblatt, J. Nitishinskaya, A. Radhakrishnan,
B. Shlegeris, and N. Thomas. Causal scrubbing, a method for rigorously testing interpretability hypotheses.
AI Alignment Forum, 2022. URL https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/cau
sal-scrubbing-a-method-for-rigorously-testing.",2022,"https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/cau
sal-scrubbing-a-method-for-rigorously-testing."
"H. Chefer, S. Gur, and L. Wolf. Transformer interpretability beyond attention visualization. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 782–791, June 2021.",2021,
"A. Chen, R. Shwartz-Ziv, K. Cho, M. L. Leavitt, and N. Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs. In The Twelfth International Conference on Learning
Representations, 2024a. URL https://openreview.net/forum?id=MO5PiKHELW.",,https://openreview.net/forum?id=MO5PiKHELW.
"C. Chen, K. Liu, Z. Chen, Y. Gu, Y. Wu, M. Tao, Z. Fu, and J. Ye. INSIDE: LLMs’ internal states retain the
power of hallucination detection. In The Twelfth International Conference on Learning Representations,
2024b. URL https://openreview.net/forum?id=Zj12nzlQbz.",,https://openreview.net/forum?id=Zj12nzlQbz.
"H. Chen, C. Vondrick, and C. Mao. Selfie: Self-interpretation of large language model embeddings, 2024c.
URL https://arxiv.org/abs/2403.10949.",,https://arxiv.org/abs/2403.10949.
"S. Chen, M. Xiong, J. Liu, Z. Wu, T. Xiao, S. Gao, and J. He. In-context sharpness as alerts: An inner
representation perspective for hallucination mitigation, 2024d.",,
"A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,
S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer,
V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari,
P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson,
L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal,
M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,
Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean,
S. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning
Research, 24(240):1–113, 2023. URL http://jmlr.org/papers/v24/22-1144.html.",2023,http://jmlr.org/papers/v24/22-1144.html.
"A. G. Chowdhury, M. M. Islam, V. Kumar, F. H. Shezan, V. Kumar, V. Jain, and A. Chadha. Breaking down
the defenses: A comparative survey of attacks on large language models, 2024.",2024,
"Y.-S. Chuang, Y. Xie, H. Luo, Y. Kim, J. R. Glass, and P. He. Dola: Decoding by contrasting layers improves
factuality in large language models. In The Twelfth International Conference on Learning Representations,
2024. URL https://openreview.net/forum?id=Th6NyL07na.",2024,https://openreview.net/forum?id=Th6NyL07na.
"B. Chughtai, A. Cooney, and N. Nanda. Summing up the facts: Additive mechanisms behind factual recall in
llms, 2024. URL https://www.arxiv.org/abs/2402.07321.",2024,https://www.arxiv.org/abs/2402.07321.
"K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does BERT look at? an analysis of BERT’s
attention. In T. Linzen, G. Chrupała, Y. Belinkov, and D. Hupkes (eds.), Proceedings of the 2019 ACL
Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276–286, Florence,
Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https:
//aclanthology.org/W19-4828.",2019,"https:
//aclanthology.org/W19-4828."
"T. Conerly, A. Templeton, T. Bricken, J. Marcus, and T. Henighan. Circuits updates - april 2024. update on
how we train saes. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024
/april-update/index.html.",2024,"https://transformer-circuits.pub/2024
/april-update/index.html."
"A. Conmy, A. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit
discovery for mechanistic interpretability. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 16318–16352. Curran
Associates, Inc., 2023. URL https://papers.nips.cc/paper_files/paper/2023/hash/34e1dbe95d3
4d7ebaf99b9bcaeb5b2be-Abstract-Conference.html.",2023,"https://papers.nips.cc/paper_files/paper/2023/hash/34e1dbe95d3
4d7ebaf99b9bcaeb5b2be-Abstract-Conference.html."
"A. Cooney. CircuitVis, December 2022. URL https://github.com/alan-cooney/CircuitsVis.",2022,https://github.com/alan-cooney/CircuitsVis.
"A. Cooney. Sparse autoencoder. GitHub repository, 2023. URL https://github.com/ai-safety-foundat
ion/sparse_autoencoder.",2023,"https://github.com/ai-safety-foundat
ion/sparse_autoencoder."
"G. M. Correia, V. Niculae, and A. F. T. Martins. Adaptively sparse transformers. In K. Inui, J. Jiang, V. Ng, and
X. Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2174–
2184, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/
D19-1223. URL https://aclanthology.org/D19-1223.",2019,https://aclanthology.org/D19-1223.
"M. Costa-jussà, E. Smith, C. Ropers, D. Licht, J. Maillard, J. Ferrando, and C. Escolano. Toxicity in
multilingual machine translation at scale. In H. Bouamor, J. Pino, and K. Bali (eds.), Findings of the
Association for Computational Linguistics: EMNLP 2023, pp. 9570–9586, Singapore, December 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.findings- emnlp.642. URL
https://aclanthology.org/2023.findings-emnlp.642.",2023,https://aclanthology.org/2023.findings-emnlp.642.
"I. Covert, S. Lundberg, and S.-I. Lee. Explaining by removing: A unified framework for model explanation.
Journal of Machine Learning Research, 22(209):1–90, 2021. URL http://jmlr.org/papers/v22/20-1
316.html.",2021,"http://jmlr.org/papers/v22/20-1
316.html."
"J. Crabbé and M. van der Schaar. Evaluating the robustness of interpretability methods through explanation
invariance and equivariance. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
URL https://openreview.net/forum?id=5UwnKSgY6u.",2023,https://openreview.net/forum?id=5UwnKSgY6u.
"R. Csordás, S. van Steenkiste, and J. Schmidhuber. Are neural nets modular? inspecting functional modularity
through differentiable weight masks. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=7uVcpu-gMD.",2021,https://openreview.net/forum?id=7uVcpu-gMD.
"H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly interpretable
features in language models. Arxiv, 2023. URL https://arxiv.org/abs/2309.08600.",2023,https://arxiv.org/abs/2309.08600.
"D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. Knowledge neurons in pretrained transformers. In S. Muresan, P. Nakov, and A. Villavicencio (eds.), Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8493–8502, Dublin, Ireland,
May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581. URL
https://aclanthology.org/2022.acl-long.581.",2022,https://aclanthology.org/2022.acl-long.581.
"D. Dale, E. Voita, L. Barrault, and M. R. Costa-jussà. Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity Even better. In A. Rogers, J. Boyd-Graber,
and N. Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 36–50, Toronto, Canada, July 2023a. Association for Computational
Linguistics. doi: 10.18653/v1/2023.acl-long.3. URL https://aclanthology.org/2023.acl-long.3.",2023,https://aclanthology.org/2023.acl-long.3.
"D. Dale, E. Voita, J. Lam, P. Hansanti, C. Ropers, E. Kalbassi, C. Gao, L. Barrault, and M. Costa-jussà.
HalOmi: A manually annotated benchmark for multilingual hallucination and omission detection in machine translation. In H. Bouamor, J. Pino, and K. Bali (eds.), Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Processing, pp. 638–653, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp- main.42. URL https:
//aclanthology.org/2023.emnlp-main.42.",2023,"https:
//aclanthology.org/2023.emnlp-main.42."
"F. Dalvi, N. Durrani, H. Sajjad, Y. Belinkov, A. Bau, and J. Glass. What is one grain of sand in the desert?
analyzing individual neurons in deep nlp models. In Proceedings of the Thirty-Third AAAI Conference
on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and
Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI’19/IAAI’19/EAAI’19.
AAAI Press, 2019. ISBN 978-1-57735-809-1. doi: 10.1609/aaai.v33i01.33016309. URL https:
//doi.org/10.1609/aaai.v33i01.33016309.",2019,"https:
//doi.org/10.1609/aaai.v33i01.33016309."
"P. A. Daniel Johnson. Penzai. GitHub repository, 2024. URL https://github.com/google-deepmind/pe
nzai.",2024,"https://github.com/google-deepmind/pe
nzai."
"J. Dao, Y.-T. Lau, C. Rager, and J. Janiak. An adversarial example for direct logit attribution: Memory management in gelu-4l, 2023.",2023,
"G. Dar, M. Geva, A. Gupta, and J. Berant. Analyzing transformers in embedding space. In A. Rogers,
J. Boyd-Graber, and N. Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 16124–16170, Toronto, Canada, July 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.acl- long.893. URL https:
//aclanthology.org/2023.acl-long.893.",2023,"https:
//aclanthology.org/2023.acl-long.893."
"T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski. Vision transformers need registers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=2dnO
3LLiJ1.",2024,"https://openreview.net/forum?id=2dnO
3LLiJ1."
"Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional networks. In
Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, pp. 933–941.
JMLR.org, 2017.",2017,
"N. De Cao, M. S. Schlichtkrull, W. Aziz, and I. Titov. How do decisions emerge across layers in neural models?
interpretation with differentiable masking. In B. Webber, T. Cohn, Y. He, and Y. Liu (eds.), Proceedings of
the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3243–3255,
Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.
262. URL https://aclanthology.org/2020.emnlp-main.262.",2020,https://aclanthology.org/2020.emnlp-main.262.
"N. De Cao, W. Aziz, and I. Titov. Editing factual knowledge in language models. In M.-F. Moens,
X. Huang, L. Specia, and S. W.-t. Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing, pp. 6491–6506, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.522. URL
https://aclanthology.org/2021.emnlp-main.522.",2021,https://aclanthology.org/2021.emnlp-main.522.
"N. De Cao, L. Schmid, D. Hupkes, and I. Titov. Sparse interventions in language models with differentiable
masking. In J. Bastings, Y. Belinkov, Y. Elazar, D. Hupkes, N. Saphra, and S. Wiegreffe (eds.), Proceedings
of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 16–27,
Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics.
doi: 10.18653/v1/2022.blackboxnlp-1.2. URL https://aclanthology.org/2022.blackboxnlp-1.2.",2022,https://aclanthology.org/2022.blackboxnlp-1.2.
"B. Deiseroth, M. Deb, S. Weinbach, M. Brack, P. Schramowski, and K. Kersting. Atman: Understanding transformer predictions through memory efficient attention manipulation. In A. Oh, T. Neumann, A. Globerson,
K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36,
pp. 63437–63460. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_fil
es/paper/2023/file/c83bc020a020cdeb966ed10804619664-Paper-Conference.pdf.",2023,"https://proceedings.neurips.cc/paper_fil
es/paper/2023/file/c83bc020a020cdeb966ed10804619664-Paper-Conference.pdf."
"M. Denil, A. Demiraj, and N. de Freitas. Extraction of salient sentences from labelled documents. Arxiv, 2015.
URL https://arxiv.org/abs/1412.6815.",2015,https://arxiv.org/abs/1412.6815.
"T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in
Neural Information Processing Systems, volume 35, pp. 30318–30332. Curran Associates, Inc., 2022. URL
https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97
e9c8a-Paper-Conference.pdf.",2022,"https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97
e9c8a-Paper-Conference.pdf."
"J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio (eds.), Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL
https://aclanthology.org/N19-1423.",2019,https://aclanthology.org/N19-1423.
"J. DeYoung, S. Jain, N. F. Rajani, E. Lehman, C. Xiong, R. Socher, and B. C. Wallace. ERASER: A benchmark
to evaluate rationalized NLP models. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault (eds.), Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4443–4458, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.408. URL https:
//aclanthology.org/2020.acl-main.408.",2020,"https:
//aclanthology.org/2020.acl-main.408."
"K. Dhamdhere, M. Sundararajan, and Q. Yan. How important is a neuron. In International Conference on
Learning Representations, 2019. URL https://openreview.net/forum?id=SylKoo0cKm.",2019,https://openreview.net/forum?id=SylKoo0cKm.
"S. Dhanorkar, C. T. Wolf, K. Qian, A. Xu, L. Popa, and Y. Li. Who needs to know what, when?: Broadening
the explainable ai (xai) design space by looking at explanations across the ai lifecycle. In Proceedings of
the 2021 ACM Designing Interactive Systems Conference, DIS ’21, pp. 1591–1602, New York, NY, USA,
2021. Association for Computing Machinery. ISBN 9781450384766. doi: 10.1145/3461778.3462131. URL
https://doi.org/10.1145/3461778.3462131.",2021,https://doi.org/10.1145/3461778.3462131.
"A. Y. Din, T. Karidi, L. Choshen, and M. Geva. Jump to conclusions: Short-cutting transformers with linear
transformations. Arxiv, 2023. URL https://arxiv.org/abs/2303.09435.",2023,https://arxiv.org/abs/2303.09435.
"S. Ding and P. Koehn. Evaluating saliency methods for neural language models. In K. Toutanova,
A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty,
and Y. Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5034–5052, Online, June
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl- main.399. URL
https://aclanthology.org/2021.naacl-main.399.",2021,https://aclanthology.org/2021.naacl-main.399.
"F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning, 2017. URL https:
//arxiv.org/abs/1702.08608.",2017,"https:
//arxiv.org/abs/1702.08608."
"N. Durrani, F. Dalvi, and H. Sajjad. Discovering salient neurons in deep nlp models. Journal of Machine
Learning Research, 24(362):1–40, 2023. URL http://jmlr.org/papers/v24/23-0074.html.",2023,http://jmlr.org/papers/v24/23-0074.html.
"Y. Elazar, S. Ravfogel, A. Jacovi, and Y. Goldberg. Amnesic probing: Behavioral explanation with amnesic
counterfactuals. Transactions of the Association for Computational Linguistics, 9:160–175, 03 2021. ISSN
2307-387X. doi: 10.1162/tacl_a_00359. URL https://doi.org/10.1162/tacl_a_00359.",2021,https://doi.org/10.1162/tacl_a_00359.
"N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly,
N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt,
K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical
framework for transformer circuits. Transformer Circuits Thread, 2021a. URL https://transformer-c
ircuits.pub/2021/framework/index.html.",2021,"https://transformer-c
ircuits.pub/2021/framework/index.html."
"N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly,
N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt,
K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. Garcon. Transformer
Circuits Thread, 2021b. URL https://transformer-circuits.pub/2021/garcon/index.html.",2021,https://transformer-circuits.pub/2021/garcon/index.html.
"N. Elhage, T. Hume, C. Olsson, N. Nanda, T. Henighan, S. Johnston, S. ElShowk, N. Joseph, N. DasSarma,
B. Mann, D. Hernandez, A. Askell, K. Ndousse, A. Jones, D. Drain, A. Chen, Y. Bai, D. Ganguli, L. Lovitt,
Z. Hatfield-Dodds, J. Kernion, T. Conerly, S. Kravec, S. Fort, S. Kadavath, J. Jacobson, E. Tran-Johnson,
J. Kaplan, J. Clark, T. Brown, S. McCandlish, D. Amodei, and C. Olah. Softmax linear units. Transformer
Circuits Thread, 2022a. URL https://transformer-circuits.pub/2022/solu/index.html.",2022,https://transformer-circuits.pub/2022/solu/index.html.
"N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain,
C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and C. Olah. Toy models of
superposition. Transformer Circuits Thread, 2022b. URL https://transformer-circuits.pub/2022/t
oy_model/index.html.",2022,"https://transformer-circuits.pub/2022/t
oy_model/index.html."
"N. Elhage, R. Lasenby, and C. Olah. Privileged bases in the transformer residual stream. Transformer Circuits
Thread, 2023. URL https://transformer-circuits.pub/2023/privileged-basis/index.html.",2023,https://transformer-circuits.pub/2023/privileged-basis/index.html.
"J. Enguehard. Sequential integrated gradients: a simple but effective method for explaining language models. In
A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 7555–7565, Toronto, Canada, July 2023. Association for Computational Linguistics.
doi: 10.18653/v1/2023.findings-acl.477. URL https://aclanthology.org/2023.findings-acl.477.",2023,https://aclanthology.org/2023.findings-acl.477.
"K. Ethayarajh. How contextual are contextualized word representations? Comparing the geometry of BERT,
ELMo, and GPT-2 embeddings. In K. Inui, J. Jiang, V. Ng, and X. Wan (eds.), Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 55–65, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19- 1006. URL https:
//aclanthology.org/D19-1006.",2019,"https:
//aclanthology.org/D19-1006."
"K. Ethayarajh and D. Jurafsky. Attention flows are shapley value explanations. In C. Zong, F. Xia, W. Li, and
R. Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics
and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp.
49–54, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.8.
URL https://aclanthology.org/2021.acl-short.8.",2021,https://aclanthology.org/2021.acl-short.8.
"N. Feldhus, L. Hennig, M. D. Nasert, C. Ebert, R. Schwarzenberg, and S. Möller. Saliency map verbalization: Comparing feature importance representations from model-free and instruction-based methods.
In B. Dalvi Mishra, G. Durrett, P. Jansen, D. Neves Ribeiro, and J. Wei (eds.), Proceedings of the 1st
Workshop on Natural Language Reasoning and Structured Explanations (NLRSE), pp. 30–46, Toronto,
Canada, June 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.nlrse-1.4. URL
https://aclanthology.org/2023.nlrse-1.4.",2023,https://aclanthology.org/2023.nlrse-1.4.
"J. Ferrando and M. R. Costa-jussà. Attention weights in transformer NMT fail aligning words between
sequences but largely explain model predictions. In M.-F. Moens, X. Huang, L. Specia, and S. W.-
t. Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 434–443,
Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.findings-emnlp.39. URL https://aclanthology.org/2021.findings-emnlp.39.",2021,https://aclanthology.org/2021.findings-emnlp.39.
"J. Ferrando and E. Voita. Information flow routes: Automatically interpreting language models at scale. Arxiv,
2024. URL https://arxiv.org/abs/2403.00824.",2024,https://arxiv.org/abs/2403.00824.
"J. Ferrando, G. I. Gállego, B. Alastruey, C. Escolano, and M. R. Costa-jussà. Towards opening the black
box of neural machine translation: Source and target interpretations of the transformer. In Y. Goldberg,
Z. Kozareva, and Y. Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, pp. 8756–8769, Abu Dhabi, United Arab Emirates, December 2022a. Association for
Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.599. URL https://aclanthology.org
/2022.emnlp-main.599.",2022,"https://aclanthology.org
/2022.emnlp-main.599."
"J. Ferrando, G. I. Gállego, and M. R. Costa-jussà. Measuring the mixing of contextual information in the
transformer. In Y. Goldberg, Z. Kozareva, and Y. Zhang (eds.), Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing, pp. 8698–8714, Abu Dhabi, United Arab Emirates,
December 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.595.
URL https://aclanthology.org/2022.emnlp-main.595.",2022,https://aclanthology.org/2022.emnlp-main.595.
"J. Ferrando, G. I. Gállego, I. Tsiamas, and M. R. Costa-jussà. Explaining how transformers use context to
build predictions. In A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.), Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5486–5513, Toronto,
Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.301. URL
https://aclanthology.org/2023.acl-long.301.",2023,https://aclanthology.org/2023.acl-long.301.
"C. Fierro and A. Søgaard. Factual consistency of multilingual pretrained language models. In S. Muresan,
P. Nakov, and A. Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022,
pp. 3046–3052, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/
2022.findings-acl.240. URL https://aclanthology.org/2022.findings-acl.240.",2022,https://aclanthology.org/2022.findings-acl.240.
"J. Fiotto-Kaufman. nnsight: The package for interpreting and manipulating the internals of deep learned models. , 2024. URL https://github.com/JadenFiotto-Kaufman/nnsight.",2024,https://github.com/JadenFiotto-Kaufman/nnsight.
"M. Fomicheva, S. Sun, L. Yankovskaya, F. Blain, F. Guzmán, M. Fishel, N. Aletras, V. Chaudhary, and
L. Specia. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539–555, 2020. doi: 10.1162/tacl_a_00330. URL https:
//aclanthology.org/2020.tacl-1.35.",2020,"https:
//aclanthology.org/2020.tacl-1.35."
"D. Friedman, A. Wettig, and D. Chen. Learning transformer programs. In A. Oh, T. Neumann, A. Globerson,
K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36.
Curran Associates, Inc., 2023. URL https://openreview.net/forum?id=Pe9WxkN8Ff.",2023,https://openreview.net/forum?id=Pe9WxkN8Ff.
"A. Geiger, K. Richardson, and C. Potts. Neural natural language inference models partially embed theories
of lexical entailment and negation. In A. Alishahi, Y. Belinkov, G. Chrupała, D. Hupkes, Y. Pinter, and
H. Sajjad (eds.), Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural
Networks for NLP, pp. 163–173, Online, November 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.blackboxnlp-1.16. URL https://aclanthology.org/2020.blackboxnlp-1.16.",2020,https://aclanthology.org/2020.blackboxnlp-1.16.
"A. Geiger, H. Lu, T. Icard, and C. Potts. Causal abstractions of neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan (eds.), Advances in Neural Information Processing Systems,
volume 34, pp. 9574–9586. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/pap
er_files/paper/2021/file/4f5c422f4d49a5a807eda27434231040-Paper.pdf.",2021,"https://proceedings.neurips.cc/pap
er_files/paper/2021/file/4f5c422f4d49a5a807eda27434231040-Paper.pdf."
"A. Geiger, Z. Wu, H. Lu, J. Rozner, E. Kreiss, T. Icard, N. D. Goodman, and C. Potts. Inducing causal structure
for interpretable neural networks, 2022. URL https://arxiv.org/abs/2112.00826.",2022,https://arxiv.org/abs/2112.00826.
"A. Geiger, C. Potts, and T. Icard. Causal abstraction for faithful model interpretation, 2023a. URL https:
//arxiv.org/abs/2301.04709.",,"https:
//arxiv.org/abs/2301.04709."
"A. Geiger, Z. Wu, C. Potts, T. Icard, and N. D. Goodman. Finding alignments between interpretable causal
variables and distributed neural representations, 2023b. URL https://arxiv.org/abs/2303.02536.",,https://arxiv.org/abs/2303.02536.
"G. Gemma Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S.
Kale, J. Love, P. Tafti, L. Hussenot, P. G. Sessa, A. Chowdhery, A. Roberts, A. Barua, A. Botev, A. CastroRos, A. Slone, A. Héliou, A. Tacchetti, A. Bulanova, A. Paterson, B. Tsai, B. Shahriari, C. L. Lan, C. A.
Choquette-Choo, C. Crepy, D. Cer, D. Ippolito, D. Reid, E. Buchatskaya, E. Ni, E. Noland, G. Yan,
G. Tucker, G.-C. Muraru, G. Rozhdestvenskiy, H. Michalewski, I. Tenney, I. Grishchenko, J. Austin, J. Keeling, J. Labanowski, J.-B. Lespiau, J. Stanway, J. Brennan, J. Chen, J. Ferret, J. Chiu, J. Mao-Jones, K. Lee,
K. Yu, K. Millican, L. L. Sjoesund, L. Lee, L. Dixon, M. Reid, M. Mikuła, M. Wirth, M. Sharman, N. Chinaev, N. Thain, O. Bachem, O. Chang, O. Wahltinez, P. Bailey, P. Michel, P. Yotov, R. Chaabouni, R. Comanescu, R. Jana, R. Anil, R. McIlroy, R. Liu, R. Mullins, S. L. Smith, S. Borgeaud, S. Girgin, S. Douglas,
S. Pandya, S. Shakeri, S. De, T. Klimenko, T. Hennigan, V. Feinberg, W. Stokowiec, Y. hui Chen, Z. Ahmed,
Z. Gong, T. Warkentin, L. Peran, M. Giang, C. Farabet, O. Vinyals, J. Dean, K. Kavukcuoglu, D. Hassabis, Z. Ghahramani, D. Eck, J. Barral, F. Pereira, E. Collins, A. Joulin, N. Fiedel, E. Senter, A. Andreev,
and K. Kenealy. Gemma: Open models based on gemini research and technology. ArXiv, 2024. URL
https://arxiv.org/abs/2403.08295.",2024,https://arxiv.org/abs/2403.08295.
"M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value memories. In M.-
F. Moens, X. Huang, L. Specia, and S. W.-t. Yih (eds.), Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pp. 5484–5495, Online and Punta Cana, Dominican Republic,
November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL
https://aclanthology.org/2021.emnlp-main.446.",2021,https://aclanthology.org/2021.emnlp-main.446.
"M. Geva, A. Caciularu, G. Dar, P. Roit, S. Sadde, M. Shlain, B. Tamir, and Y. Goldberg. LM-debugger: An interactive tool for inspection and intervention in transformer-based language models. In W. Che and E. Shutova
(eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations, pp. 12–21, Abu Dhabi, UAE, December 2022a. Association for Computational Linguistics.
doi: 10.18653/v1/2022.emnlp-demos.2. URL https://aclanthology.org/2022.emnlp-demos.2.",2022,https://aclanthology.org/2022.emnlp-demos.2.
"M. Geva, A. Caciularu, K. Wang, and Y. Goldberg. Transformer feed-forward layers build predictions by
promoting concepts in the vocabulary space. In Y. Goldberg, Z. Kozareva, and Y. Zhang (eds.), Proceedings
of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 30–45, Abu Dhabi,
United Arab Emirates, December 2022b. Association for Computational Linguistics. doi: 10.18653/v1/20
22.emnlp-main.3. URL https://aclanthology.org/2022.emnlp-main.3.",2022,https://aclanthology.org/2022.emnlp-main.3.
"M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in autoregressive language models. In H. Bouamor, J. Pino, and K. Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 12216–12235, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.751. URL
https://aclanthology.org/2023.emnlp-main.751.",2023,https://aclanthology.org/2023.emnlp-main.751.
"A. Ghandeharioun, A. Caciularu, A. Pearce, L. Dixon, and M. Geva. Patchscopes: A unifying framework for
inspecting hidden representations of language models. Arxiv, 2024. URL https://arxiv.org/abs/2401
.06102v2.",2024,"https://arxiv.org/abs/2401
.06102v2."
"N. Goldowsky-Dill, C. MacLeod, L. Sato, and A. Arora. Localizing model behavior with path patching. Arxiv,
2023. URL https://arxiv.org/abs/2304.05969.",2023,https://arxiv.org/abs/2304.05969.
"R. Gould, E. Ong, G. Ogden, and A. Conmy. Successor heads: Recurring, interpretable attention heads in the
wild. In The Twelfth International Conference on Learning Representations, 2024. URL https://openre
view.net/forum?id=kvcbV8KQsi.",2024,"https://openre
view.net/forum?id=kvcbV8KQsi."
"D. Groeneveld, I. Beltagy, P. Walsh, A. Bhagia, R. Kinney, O. Tafjord, A. H. Jha, H. Ivison, I. Magnusson,
Y. Wang, S. Arora, D. Atkinson, R. Authur, K. R. Chandu, A. Cohan, J. Dumas, Y. Elazar, Y. Gu, J. Hessel,
T. Khot, W. Merrill, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, V. Pyatkin, A. Ravichander, D. Schwenk, S. Shah, W. Smith, E. Strubell, N. Subramani, M. Wortsman, P. Dasigi, N. Lambert,
K. Richardson, L. Zettlemoyer, J. Dodge, K. Lo, L. Soldaini, N. A. Smith, and H. Hajishirzi. Olmo: Accelerating the science of language models, 2024.",2024,
"R. B. Grosse, J. Bae, C. Anil, N. Elhage, A. Tamkin, A. Tajdini, B. Steiner, D. Li, E. Durmus, E. Perez,
E. Hubinger, K. Lukovsiut.e, K. Nguyen, N. Joseph, S. McCandlish, J. Kaplan, and S. Bowman. Studying
large language model generalization with influence functions. ArXiv, abs/2308.03296, 2023. URL https:
//api.semanticscholar.org/CorpusID:260682872.",2023,"https:
//api.semanticscholar.org/CorpusID:260682872."
"A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.",2023,
"J.-C. Gu, H. Xu, J.-Y. Ma, P. Lu, Z.-H. Ling, K. wei Chang, and N. Peng. Model editing can hurt general
abilities of large language models. ArXiv, abs/2401.04700, 2024. URL https://api.semanticscholar.
org/CorpusID:266899568.",2024,"https://api.semanticscholar.
org/CorpusID:266899568."
"C. Guerner, A. Svete, T. Liu, A. Warstadt, and R. Cotterell. A geometric notion of causal probing, 2023.",2023,
"N. M. Guerreiro, P. Colombo, P. Piantanida, and A. Martins. Optimal transport for unsupervised hallucination
detection in neural machine translation. In A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.), Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
13766–13784, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/
2023.acl-long.770. URL https://aclanthology.org/2023.acl-long.770.",2023,https://aclanthology.org/2023.acl-long.770.
"N. M. Guerreiro, E. Voita, and A. Martins. Looking for a needle in a haystack: A comprehensive study of
hallucinations in neural machine translation. In A. Vlachos and I. Augenstein (eds.), Proceedings of the
17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 1059–1075,
Dubrovnik, Croatia, May 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-m
ain.75. URL https://aclanthology.org/2023.eacl-main.75.",2023,https://aclanthology.org/2023.eacl-main.75.
"A. Gupta, G. Boleda, M. Baroni, and S. Padó. Distributional vectors encode referential attributes. In
L. Màrquez, C. Callison-Burch, and J. Su (eds.), Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pp. 12–21, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1002. URL https://aclanthology.org/D15-1002.",2015,https://aclanthology.org/D15-1002.
"A. Gupta, A. Rao, and G. K. Anumanchipalli. Model editing at scale leads to gradual and catastrophic forgetting. ArXiv, abs/2401.07453, 2024a. URL https://api.semanticscholar.org/CorpusID:266999650.",,https://api.semanticscholar.org/CorpusID:266999650.
"A. Gupta, D. Sajnani, and G. Anumanchipalli. A unified framework for model editing, 2024b.",,
"W. Gurnee. Sae reconstruction errors are (empirically) pathological. AI Alignment Forum, 2024. URL https:
//www.alignmentforum.org/posts/rZPiuFxESMxCDHe4B/sae-reconstruction-errors-are-empir
ically-pathological.",2024,"https:
//www.alignmentforum.org/posts/rZPiuFxESMxCDHe4B/sae-reconstruction-errors-are-empir
ically-pathological."
"W. Gurnee and M. Tegmark. Language models represent space and time. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=jE8xbmvFin.",2024,https://openreview.net/forum?id=jE8xbmvFin.
"W. Gurnee, N. Nanda, M. Pauly, K. Harvey, D. Troitskii, and D. Bertsimas. Finding neurons in a haystack:
Case studies with sparse probing. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.
URL https://openreview.net/forum?id=JYs1R9IMJr.",2023,https://openreview.net/forum?id=JYs1R9IMJr.
"W. Gurnee, T. Horsley, Z. C. Guo, T. R. Kheirkhah, Q. Sun, W. Hathaway, N. Nanda, and D. Bertsimas.
Universal neurons in gpt2 language models, 2024.",2024,
"K. Guu, A. Webson, E. Pavlick, L. Dixon, I. Tenney, and T. Bolukbasi. Simfluence: Modeling the influence of
individual training examples by simulating training runs. Arxiv, 2023. URL https://arxiv.org/abs/23
03.08114.",2023,"https://arxiv.org/abs/23
03.08114."
"Z. Hammoudeh and D. Lowd. Training data influence analysis and estimation: A survey, 2022.",2022,
"X. Han, B. C. Wallace, and Y. Tsvetkov. Explaining black box predictions and unveiling data artifacts
through influence functions. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault (eds.), Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5553–5563, Online,
July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.492. URL
https://aclanthology.org/2020.acl-main.492.",2020,https://aclanthology.org/2020.acl-main.492.
"Z. Han, C. Gao, J. Liu, J. Zhang, and S. Q. Zhang. Parameter-efficient fine-tuning for large models: A comprehensive survey, 2024.",2024,
"M. Hanna, O. Liu, and A. Variengien. How does GPT-2 compute greater-than?: Interpreting mathematical
abilities in a pre-trained language model. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 76033–76060. Curran
Associates, Inc., 2023. URL https://papers.nips.cc/paper_files/paper/2023/hash/efbba7719cc
5172d175240f24be11280-Abstract-Conference.html.",2023,"https://papers.nips.cc/paper_files/paper/2023/hash/efbba7719cc
5172d175240f24be11280-Abstract-Conference.html."
"M. Hanna, S. Pezzelle, and Y. Belinkov. Have faith in faithfulness: Going beyond circuit overlap when finding
model mechanisms, 2024. URL https://arxiv.org/abs/2403.17806.",2024,https://arxiv.org/abs/2403.17806.
"P. Hase, M. Bansal, B. Kim, and A. Ghandeharioun. Does localization inform editing? surprising differences in
causality-based localization vs. knowledge editing in language models. In A. Oh, T. Neumann, A. Globerson,
K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36,
pp. 17643–17668. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_fil
es/paper/2023/file/3927bbdcf0e8d1fa8aa23c26f358a281-Paper-Conference.pdf.",2023,"https://proceedings.neurips.cc/paper_fil
es/paper/2023/file/3927bbdcf0e8d1fa8aa23c26f358a281-Paper-Conference.pdf."
"A. Haviv, I. Cohen, J. Gidron, R. Schuster, Y. Goldberg, and M. Geva. Understanding transformer memorization
recall through idioms. In A. Vlachos and I. Augenstein (eds.), Proceedings of the 17th Conference of the
European Chapter of the Association for Computational Linguistics, pp. 248–264, Dubrovnik, Croatia, May
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.19. URL https:
//aclanthology.org/2023.eacl-main.19.",2023,"https:
//aclanthology.org/2023.eacl-main.19."
"Z. He, X. Ge, Q. Tang, T. Sun, Q. Cheng, and X. Qiu. Dictionary learning improves patch-free circuit discovery
in mechanistic interpretability: A case study on othello-gpt. ArXiv, abs/2402.12201, 2024. URL https:
//api.semanticscholar.org/CorpusID:267751496.",2024,"https:
//api.semanticscholar.org/CorpusID:267751496."
"S. Heimersheim and J. Janiak. A circuit for python docstrings in a 4-layer attention-only transformer. AI
Alignment Forum, 2023. URL https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-cir
cuit-for-python-docstrings-in-a-4-layer-attention-only.",2023,"https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-cir
cuit-for-python-docstrings-in-a-4-layer-attention-only."
"S. Heimersheim and N. Nanda. How to use and interpret activation patching. Arxiv, 2024. URL https:
//arxiv.org/abs/2404.15255.",2024,"https:
//arxiv.org/abs/2404.15255."
"S. Heimersheim and A. Turner. Residual stream norms grow exponentially over the forward pass. AI Alignment
Forum, 2023. URL https://www.alignmentforum.org/posts/8mizBCm3dyc432nK8/residual-strea
m-norms-grow-exponentially-over-the-forward.",2023,"https://www.alignmentforum.org/posts/8mizBCm3dyc432nK8/residual-strea
m-norms-grow-exponentially-over-the-forward."
"R. Hendel, M. Geva, and A. Globerson. In-context learning creates task vectors. Arxiv, 2023. URL https:
//arxiv.org/abs/2310.15916.",2023,"https:
//arxiv.org/abs/2310.15916."
"E. Hernandez, A. S. Sharma, T. Haklay, K. Meng, M. Wattenberg, J. Andreas, Y. Belinkov, and D. Bau. Linearity of relation decoding in transformer language models. In The Twelfth International Conference on
Learning Representations, 2024. URL https://openreview.net/forum?id=w7LU2s14kE.",2024,https://openreview.net/forum?id=w7LU2s14kE.
"J. Hewitt and P. Liang. Designing and interpreting probes with control tasks. In K. Inui, J. Jiang, V. Ng, and
X. Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2733–
2743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/
D19-1275. URL https://aclanthology.org/D19-1275.",2019,https://aclanthology.org/D19-1275.
"J. Hewitt and C. D. Manning. A structural probe for finding syntax in word representations. In J. Burstein,
C. Doran, and T. Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pp. 4129–4138, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
doi: 10.18653/v1/N19-1419. URL https://aclanthology.org/N19-1419.",2019,https://aclanthology.org/N19-1419.
"J. Hewitt, J. Thickstun, C. Manning, and P. Liang. Backpack language models. In A. Rogers, J. Boyd-Graber,
and N. Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9103–9125, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.506. URL https://aclanthology.org/2023.acl-l
ong.506.",2023,"https://aclanthology.org/2023.acl-l
ong.506."
"A. Himmi, G. Staerman, M. Picot, P. Colombo, and N. M. Guerreiro. Enhanced hallucination detection in
neural machine translation through simple detector aggregation, 2024.",2024,
"J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy,
S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre. An empirical analysis of computeoptimal large language model training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and
A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 30016–30030. Curran
Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/hash/c1e
2faff6f588870935f114ebe04a3e5-Abstract-Conference.html.",2022,"https://proceedings.neurips.cc/paper_files/paper/2022/hash/c1e
2faff6f588870935f114ebe04a3e5-Abstract-Conference.html."
"A. Holtzman, P. West, V. Shwartz, Y. Choi, and L. Zettlemoyer. Surface form competition: Why the highest
probability answer isn’t always right. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7038–7051,
Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
doi: 10.18653/v1/2021.emnlp-main.564. URL https://aclanthology.org/2021.emnlp-main.564.",2021,https://aclanthology.org/2021.emnlp-main.564.
"B. Hoover, H. Strobelt, and S. Gehrmann. exBERT: A Visual Analysis Tool to Explore Learned Representations
in Transformer Models. In A. Celikyilmaz and T.-H. Wen (eds.), Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics: System Demonstrations, pp. 187–196, Online, July 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-demos.22. URL https://aclant
hology.org/2020.acl-demos.22.",2020,"https://aclant
hology.org/2020.acl-demos.22."
"P. M. Htut, J. Phang, S. Bordia, and S. R. Bowman. Do attention heads in bert track syntactic dependencies?
Arxiv, 2019. URL https://arxiv.org/abs/1911.12246.",2019,https://arxiv.org/abs/1911.12246.
"J. Huang, A. Geiger, K. D’Oosterlinck, Z. Wu, and C. Potts. Rigorously assessing natural language explanations
of neurons. In Y. Belinkov, S. Hao, J. Jumelet, N. Kim, A. McCarthy, and H. Mohebbi (eds.), Proceedings
of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 317–331,
Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxn
lp-1.24. URL https://aclanthology.org/2023.blackboxnlp-1.24.",2023,https://aclanthology.org/2023.blackboxnlp-1.24.
"J. Huang, Z. Wu, C. Potts, M. Geva, and A. Geiger. Ravel: Evaluating interpretability methods on disentangling
language model representations, 2024a. URL https://arxiv.org/abs/2402.17700.",,https://arxiv.org/abs/2402.17700.
"Y. Huang, S. Hu, X. Han, Z. Liu, and M. Sun. Unified view of grokking, double descent and emergent abilities:
A perspective from circuits competition, 2024b.",,
"N. Hudson, J. G. Pauloski, M. Baughman, A. Kamatar, M. Sakarvadia, L. Ward, R. Chard, A. Bauer,
M. Levental, W. Wang, W. Engler, O. P. Skelly, B. Blaiszik, R. Stevens, K. Chard, and I. Foster. Trillion parameter ai serving infrastructure for scientific discovery: A survey and vision. Arxiv, 2024. URL
https://arxiv.org/abs/2402.03480.",2024,https://arxiv.org/abs/2402.03480.
"D. Hupkes, S. Veldhoen, and W. Zuidema. Visualisation and ‘diagnostic classifiers’ reveal how recurrent and
recursive neural networks process hierarchical structure. Journal of Artificial Intelligence Research, 61(1):
907–926, 2018. ISSN 1076-9757.",2018,
"S. Jain, R. Kirk, E. S. Lubana, R. P. Dick, H. Tanaka, T. Rocktäschel, E. Grefenstette, and D. Krueger. What
happens when you fine-tuning your model? mechanistic analysis of procedurally generated tasks. In The
Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/f
orum?id=A0HKeKl4Nl.",2024,"https://openreview.net/f
orum?id=A0HKeKl4Nl."
"S. Jain and B. C. Wallace. Attention is not Explanation. In J. Burstein, C. Doran, and T. Solorio (eds.),
Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3543–3556, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1357. URL
https://aclanthology.org/N19-1357.",2019,https://aclanthology.org/N19-1357.
"S. Jastrz˛ebski, D. Arpit, N. Ballas, V. Verma, T. Che, and Y. Bengio. Residual connections encourage iterative
inference, 2018.",2018,
"A. Jermyn and A. Templeton. Circuits updates - jnauary 2024. ghost grads: An improvement on resampling.
Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/jan-update/in
dex.html.",2024,"https://transformer-circuits.pub/2024/jan-update/in
dex.html."
"Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung. Survey of
hallucination in natural language generation. ACM Computing Surveys, 55(12), mar 2023. ISSN 0360-0300.
doi: 10.1145/3571730. URL https://doi.org/10.1145/3571730.",2023,https://doi.org/10.1145/3571730.
"Y. Jiang, G. Rajendran, P. Ravikumar, B. Aragam, and V. Veitch. On the origins of linear representations in
large language models, 2024.",2024,
"S. Joseph. Vit prisma: A mechanistic interpretability library for vision transformers. GitHub repository, 2023.
URL https://github.com/soniajoseph/vit-prisma.",2023,https://github.com/soniajoseph/vit-prisma.
"G. Kamradt. Needle in a haystack - pressure testing llms. Github Repository, 2023. URL https://github.c
om/gkamradt/LLMTest_NeedleInAHaystack.",2023,"https://github.c
om/gkamradt/LLMTest_NeedleInAHaystack."
"J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and
D. Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361.",2020,https://arxiv.org/abs/2001.08361.
"S. Katz and Y. Belinkov. VISIT: Visualizing and interpreting the semantic information flow of transformers. In
H. Bouamor, J. Pino, and K. Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP
2023, pp. 14094–14113, Singapore, December 2023. Association for Computational Linguistics. doi: 10.1
8653/v1/2023.findings-emnlp.939. URL https://aclanthology.org/2023.findings-emnlp.939.",2023,https://aclanthology.org/2023.findings-emnlp.939.
"S. Katz, Y. Belinkov, M. Geva, and L. Wolf. Backward lens: Projecting language model gradients into the
vocabulary space, 2024.",2024,
"B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, and R. sayres. Interpretability beyond feature
attribution: Quantitative testing with concept activation vectors (TCAV). In J. Dy and A. Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 2668–2677. PMLR, 2018. URL https://proceedings.mlr.press/v80/kim18d
.html.",2018,"https://proceedings.mlr.press/v80/kim18d
.html."
"C. Kissane, R. Krzyzanowski, A. Conmy, and N. Nanda. Sparse autoencoders work on attention layer outputs.
AI Alignment Forum, 2024a. URL https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZ.",,https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZ.
"C. Kissane, R. Krzyzanowski, A. Conmy, and N. Nanda. Attention saes scale to gpt-2 small. AI Alignment
Forum, 2024b. URL https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr/attention-sae
s-scale-to-gpt-2-small.",,"https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr/attention-sae
s-scale-to-gpt-2-small."
"G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Attention is not only a weight: Analyzing transformers
with vector norms. In B. Webber, T. Cohn, Y. He, and Y. Liu (eds.), Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pp. 7057–7075, Online, November 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.574. URL https:
//aclanthology.org/2020.emnlp-main.574.",2020,"https:
//aclanthology.org/2020.emnlp-main.574."
"G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Incorporating Residual and Normalization Layers into
Analysis of Masked Language Models. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih (eds.),
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 4547–4568,
Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
doi: 10.18653/v1/2021.emnlp-main.373. URL https://aclanthology.org/2021.emnlp-main.373.",2021,https://aclanthology.org/2021.emnlp-main.373.
"G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Transformer language models handle word frequency
in prediction head. In A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.), Findings of the Association for
Computational Linguistics: ACL 2023, pp. 4523–4535, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.276. URL https://aclanthology.org/202
3.findings-acl.276.",2023,"https://aclanthology.org/202
3.findings-acl.276."
"G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Analyzing feed-forward blocks in transformers through
the lens of attention map. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=mYWsyTuiRp.",2024,https://openreview.net/forum?id=mYWsyTuiRp.
"P. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In D. Precup and
Y. W. Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pp. 1885–1894. PMLR, 06–11 Aug 2017. URL https://pr
oceedings.mlr.press/v70/koh17a.html.",2017,"https://pr
oceedings.mlr.press/v70/koh17a.html."
"A. Köhn. What’s in an embedding? analyzing word embeddings through multilingual evaluation. In
L. Màrquez, C. Callison-Burch, and J. Su (eds.), Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pp. 2067–2073, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1246. URL https://aclanthology.org/D15-1246.",2015,https://aclanthology.org/D15-1246.
"N. Kokhlikyan, V. Miglani, M. Martin, E. Wang, B. Alsallakh, J. Reynolds, A. Melnikov, N. Kliushkina,
C. Araya, S. Yan, and O. Reblitz-Richardson. Captum: A unified and generic model interpretability library
for pytorch. Arxiv, 2020. URL https://arxiv.org/abs/2009.07896.",2020,https://arxiv.org/abs/2009.07896.
"O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky. Revealing the dark secrets of BERT. In K. Inui,
J. Jiang, V. Ng, and X. Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 4365–4374, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-1445. URL https://aclanthology.org/D19-1445.",2019,https://aclanthology.org/D19-1445.
"O. Kovaleva, S. Kulshreshtha, A. Rogers, and A. Rumshisky. BERT busters: Outlier dimensions that disrupt
transformers. In C. Zong, F. Xia, W. Li, and R. Navigli (eds.), Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021, pp. 3392–3405, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.300. URL https://aclanthology.org/2021.findings-a
cl.300.",2021,"https://aclanthology.org/2021.findings-a
cl.300."
"J. Kramár, T. Lieberum, R. Shah, and N. Nanda. Atp*: An efficient and scalable method for localizing llm
behaviour to components, 2024. URL https://arxiv.org/abs/2403.00745.",2024,https://arxiv.org/abs/2403.00745.
"R. Krzyzanowski, C. Kissane, A. Conmy, and N. Nanda. We inspected every head in GPT-2 small using saes
so you don’t have to. AI Alignment Forum, 2024. URL https://www.alignmentforum.org/posts/xme
geW5mqiBsvoaim/we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don.",2024,"https://www.alignmentforum.org/posts/xme
geW5mqiBsvoaim/we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don."
"Y. Kwon, E. Wu, K. Wu, and J. Zou. Datainf: Efficiently estimating data influence in loRA-tuned LLMs
and diffusion models. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=9m02ib92Wz.",2024,https://openreview.net/forum?id=9m02ib92Wz.
"V. Lal, A. Ma, E. Aflalo, P. Howard, A. Simoes, D. Korat, O. Pereg, G. Singer, and M. Wasserblat. InterpreT: An
interactive visualization tool for interpreting transformers. In D. Gkatzia and D. Seddah (eds.), Proceedings
of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System
Demonstrations, pp. 135–142, Online, April 2021. Association for Computational Linguistics. doi: 10.186
53/v1/2021.eacl-demos.17. URL https://aclanthology.org/2021.eacl-demos.17.",2021,https://aclanthology.org/2021.eacl-demos.17.
"A. Langedijk, H. Mohebbi, G. Sarti, W. Zuidema, and J. Jumelet. Decoderlens: Layerwise interpretation of
encoder-decoder transformers. ArXiv, abs/2310.03686, 2023. URL https://api.semanticscholar.org/
CorpusID:263671583.",2023,"https://api.semanticscholar.org/
CorpusID:263671583."
"T. Lanham, A. Chen, A. Radhakrishnan, B. Steiner, C. E. Denison, D. Hernandez, D. Li, E. Durmus, E. Hubinger, J. Kernion, K. Lukovsiut.e, K. Nguyen, N. Cheng, N. Joseph, N. Schiefer, O. Rausch, R. Larson,
S. McCandlish, S. Kundu, S. Kadavath, S. Yang, T. Henighan, T. D. Maxwell, T. Telleen-Lawton, T. Hume,
Z. Hatfield-Dodds, J. Kaplan, J. Brauner, S. Bowman, and E. Perez. Measuring faithfulness in chain-ofthought reasoning. ArXiv, abs/2307.13702, 2023. URL https://api.semanticscholar.org/CorpusID:
259953372.",2023,"https://api.semanticscholar.org/CorpusID:
259953372."
"K. Leino, S. Sen, A. Datta, M. Fredrikson, and L. Li. Influence-directed explanations for deep convolutional
networks. In 2018 IEEE International Test Conference (ITC), pp. 1–8, 2018. doi: 10.1109/TEST.2018.86
24792.",2018,
"M. A. Lepori, T. Serre, and E. Pavlick. Uncovering intermediate variables in transformers using circuit probing,
2023.",2023,
"J. Li, X. Chen, E. Hovy, and D. Jurafsky. Visualizing and understanding neural models in NLP. In K. Knight,
A. Nenkova, and O. Rambow (eds.), Proceedings of the 2016 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, pp. 681–691, San Diego,
California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1082. URL
https://aclanthology.org/N16-1082.",2016,https://aclanthology.org/N16-1082.
"J. Li, W. Monroe, and D. Jurafsky. Understanding neural networks through representation erasure, 2017.",2017,
"K. Li, O. Patel, F. Viégas, H. Pfister, and M. Wattenberg. Inference-time intervention: Eliciting truthful answers
from a language model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.
URL https://openreview.net/forum?id=aLLuYpn83y.",,https://openreview.net/forum?id=aLLuYpn83y.
"X. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis. Contrastive
decoding: Open-ended text generation as optimization. In A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.),
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 12286–12312, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi:
10.18653/v1/2023.acl-long.687. URL https://aclanthology.org/2023.acl-long.687.",2023,https://aclanthology.org/2023.acl-long.687.
"Z. Li, N. Zhang, Y. Yao, M. Wang, X. Chen, and H. Chen. Unveiling the pitfalls of knowledge editing for
large language models. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=fNktD3ib16.",2024,https://openreview.net/forum?id=fNktD3ib16.
"Q. V. Liao, D. Gruen, and S. Miller. Questioning the ai: Informing design practices for explainable ai user
experiences. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI
’20, pp. 1–15, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450367080.
doi: 10.1145/3313831.3376590. URL https://doi.org/10.1145/3313831.3376590.",2020,https://doi.org/10.1145/3313831.3376590.
"T. Lieberum, M. Rahtz, J. Kramár, N. Nanda, G. Irving, R. Shah, and V. Mikulik. Does circuit analysis
interpretability scale? evidence from multiple choice capabilities in chinchilla. Arxiv, 2023. URL https:
//arxiv.org/abs/2307.09458.",2023,"https:
//arxiv.org/abs/2307.09458."
"J. Lin and J. Bloom. Announcing neuronpedia: Platform for accelerating research into sparse autoencoders. AI
Alignment Forum, 2024. URL https://www.alignmentforum.org/posts/BaEQoxHhWPrkinmxd/annou
ncing-neuronpedia-platform-for-accelerating-research.",2024,"https://www.alignmentforum.org/posts/BaEQoxHhWPrkinmxd/annou
ncing-neuronpedia-platform-for-accelerating-research."
"Y. Lin, Y. C. Tan, and R. Frank. Open sesame: Getting inside BERT’s linguistic knowledge. In T. Linzen,
G. Chrupała, Y. Belinkov, and D. Hupkes (eds.), Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 241–253, Florence, Italy, August 2019. Association for
Computational Linguistics. doi: 10.18653/v1/W19-4825. URL https://aclanthology.org/W19-4825.",2019,https://aclanthology.org/W19-4825.
"D. Lindner, J. Kramar, S. Farquhar, M. Rahtz, T. McGrath, and V. Mikulik. Tracr: Compiled transformers as
a laboratory for interpretability. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine
(eds.), Advances in Neural Information Processing Systems, volume 36, pp. 37876–37899. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/771155a
baae744e08576f1f3b4b7ac0d-Paper-Conference.pdf.",2023,"https://proceedings.neurips.cc/paper_files/paper/2023/file/771155a
baae744e08576f1f3b4b7ac0d-Paper-Conference.pdf."
"Z. C. Lipton. The mythos of model interpretability: In machine learning, the concept of interpretability is both
important and slippery. Queue, 16(3):31–57, jun 2018. ISSN 1542-7730. doi: 10.1145/3236386.3241340.
URL https://doi.org/10.1145/3236386.3241340.",2018,https://doi.org/10.1145/3236386.3241340.
"N. F. Liu, M. Gardner, Y. Belinkov, M. E. Peters, and N. A. Smith. Linguistic knowledge and transferability of contextual representations. In J. Burstein, C. Doran, and T. Solorio (eds.), Proceedings of
the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1073–1094, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1112. URL
https://aclanthology.org/N19-1112.",2019,https://aclanthology.org/N19-1112.
"Q. Liu, Y. Chai, S. Wang, Y. Sun, K. Wang, and H. Wu. On training data influence of gpt models. Arxiv, 2024.
URL https://arxiv.org/abs/2404.07840.",2024,https://arxiv.org/abs/2404.07840.
"L. Longo, M. Brcic, F. Cabitza, J. Choi, R. Confalonieri, J. D. Ser, R. Guidotti, Y. Hayashi, F. Herrera,
A. Holzinger, R. Jiang, H. Khosravi, F. Lecue, G. Malgieri, A. Páez, W. Samek, J. Schneider, T. Speith,
and S. Stumpf. Explainable artificial intelligence (xai) 2.0: A manifesto of open challenges and interdisciplinary research directions. Information Fusion, 106:102301, 2024. ISSN 1566-2535. doi:
https://doi.org/10.1016/j.inffus.2024.102301. URL https://www.sciencedirect.com/science/
article/pii/S1566253524000794.",2024,"https://www.sciencedirect.com/science/
article/pii/S1566253524000794."
"M. Loog, T. Viering, A. Mey, J. H. Krijthe, and D. M. J. Tax. A brief prehistory of double descent. Proceedings
of the National Academy of Sciences, 117(20):10625–10626, 2020. doi: 10.1073/pnas.2001875117. URL
https://www.pnas.org/doi/abs/10.1073/pnas.2001875117.",2020,https://www.pnas.org/doi/abs/10.1073/pnas.2001875117.
"S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc
/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf.",2017,"https://proceedings.neurips.cc
/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf."
"Z. Luo, A. Kulmizev, and X. Mao. Positional artefacts propagate through masked language model embeddings.
In C. Zong, F. Xia, W. Li, and R. Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pp. 5312–5327, Online, August 2021. Association for Computational Linguistics.
doi: 10.18653/v1/2021.acl-long.413. URL https://aclanthology.org/2021.acl-long.413.",2021,https://aclanthology.org/2021.acl-long.413.
"A. Lv, K. Zhang, Y. Chen, Y. Wang, L. Liu, J.-R. Wen, J. Xie, and R. Yan. Interpreting key mechanisms of
factual recall in transformer-based language models. Computing Research Repository, arXiv:2403.19521,
2024. URL https://arxiv.org/abs/2403.19521.",2024,https://arxiv.org/abs/2403.19521.
"Q. Lyu, M. Apidianaki, and C. Callison-Burch. Towards Faithful Model Explanation in NLP: A Survey.
Computational Linguistics, pp. 1–70, 01 2024. ISSN 0891-2017. doi: 10.1162/coli_a_00511. URL
https://doi.org/10.1162/coli_a_00511.",2024,https://doi.org/10.1162/coli_a_00511.
"M. MacDiarmid, T. Maxwell, N. Schiefer, J. Mu, J. Kaplan, D. Duvenaud, S. Bowman, A. Tamkin, E. Perez,
M. Sharma, C. Denison, and E. Hubinger. Simple probes can catch sleeper agents. Anthropic, 2024. URL
https://www.anthropic.com/news/probes-catch-sleeper-agents.",2024,https://www.anthropic.com/news/probes-catch-sleeper-agents.
"A. Madsen, S. Reddy, and S. Chandar. Post-hoc interpretability for neural nlp: A survey. ACM Computing
Surveys, 55(8), 2022. ISSN 0360-0300. doi: 10.1145/3546577. URL https://doi.org/10.1145/354657
7.",2022,"https://doi.org/10.1145/354657
7."
"A. Madsen, S. Chandar, and S. Reddy. Are self-explanations from large language models faithful? ArXiv,
abs/2401.07927, 2024. URL https://api.semanticscholar.org/CorpusID:266999774.",2024,https://api.semanticscholar.org/CorpusID:266999774.
"A. Makelov, G. Lange, A. Geiger, and N. Nanda. Is this the subspace you are looking for? an interpretability
illusion for subspace activation patching. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=Ebt7JgMHv1.",2024,https://openreview.net/forum?id=Ebt7JgMHv1.
"S. Marks and A. Mueller. Dictionary learning. GitHub repository, 2023. URL https://github.com/saprm
arks/dictionary_learning.",2023,"https://github.com/saprm
arks/dictionary_learning."
"S. Marks and M. Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets, 2023. URL https://arxiv.org/abs/2310.06824.",2023,https://arxiv.org/abs/2310.06824.
"S. Marks, C. Rager, E. J. Michaud, Y. Belinkov, D. Bau, and A. Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. Computing Research Repository,
arXiv:2403.19647, 2024. URL https://arxiv.org/abs/2403.19647.",2024,https://arxiv.org/abs/2403.19647.
"T. McCoy, E. Pavlick, and T. Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural
language inference. In A. Korhonen, D. Traum, and L. Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3428–3448, Florence, Italy, July 2019. Association
for Computational Linguistics. doi: 10.18653/v1/P19-1334. URL https://aclanthology.org/P19-1334.",2019,https://aclanthology.org/P19-1334.
"C. McDougall. Six (and a half) intuitions for SVD. Callum McDougall Blog, 2023. URL https://www.perf
ectlynormal.co.uk/blog-kl-divergence.",2023,"https://www.perf
ectlynormal.co.uk/blog-kl-divergence."
"C. McDougall and J. Bloom. Sae-vis: Announcement post. LessWrong, 2024. URL https://www.lesswron
g.com/posts/nAhy6ZquNY7AD3RkD/sae-vis-announcement-post-1.",2024,"https://www.lesswron
g.com/posts/nAhy6ZquNY7AD3RkD/sae-vis-announcement-post-1."
"C. McDougall, A. Conmy, C. Rushing, T. McGrath, and N. Nanda. Copy suppression: Comprehensively
understanding an attention head. Arxiv, 2023. URL https://arxiv.org/abs/2310.04625.",2023,https://arxiv.org/abs/2310.04625.
"T. McGrath, A. Kapishnikov, N. Tomašev, A. Pearce, M. Wattenberg, D. Hassabis, B. Kim, U. Paquet, and
V. Kramnik. Acquisition of chess knowledge in alphazero. Proceedings of the National Academy of Sciences,
119(47):e2206625119, 2022. doi: 10.1073/pnas.2206625119. URL https://www.pnas.org/doi/abs/1
0.1073/pnas.2206625119.",2022,"https://www.pnas.org/doi/abs/1
0.1073/pnas.2206625119."
"T. McGrath, M. Rahtz, J. Kramar, V. Mikulik, and S. Legg. The hydra effect: Emergent self-repair in language
model computations. Arxiv, 2023. URL https://arxiv.org/abs/2307.15771.",2023,https://arxiv.org/abs/2307.15771.
"K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in GPT. In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural
Information Processing Systems, volume 35, pp. 17359–17372. Curran Associates, Inc., 2022. URL
https://proceedings.neurips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b066
5b33bf3a182-Abstract-Conference.html.",2022,"https://proceedings.neurips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b066
5b33bf3a182-Abstract-Conference.html."
"K. Meng, A. S. Sharma, A. J. Andonian, Y. Belinkov, and D. Bau. Mass-editing memory in a transformer. In
The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.n
et/forum?id=MkbcAHIYgyS.",2023,"https://openreview.n
et/forum?id=MkbcAHIYgyS."
"W. Merrill, V. Ramanujan, Y. Goldberg, R. Schwartz, and N. A. Smith. Effects of parameter norm growth
during transformer training: Inductive bias from gradient descent. In M.-F. Moens, X. Huang, L. Specia,
and S. W.-t. Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pp. 1766–1781, Online and Punta Cana, Dominican Republic, November 2021. Association for
Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.133. URL https://aclanthology.org
/2021.emnlp-main.133.",2021,"https://aclanthology.org
/2021.emnlp-main.133."
"W. Merrill, N. Tsilivis, and A. Shukla. A tale of two circuits: Grokking as competition of sparse and dense
subnetworks. ArXiv, abs/2303.11873, 2023. URL https://api.semanticscholar.org/CorpusID:
257636667.",2023,"https://api.semanticscholar.org/CorpusID:
257636667."
"J. Merullo, C. Eickhoff, and E. Pavlick. A mechanism for solving relational tasks in transformer language
models, 2023. URL https://arxiv.org/abs/2305.16130.",2023,https://arxiv.org/abs/2305.16130.
"J. Merullo, C. Eickhoff, and E. Pavlick. Circuit component reuse across tasks in transformer language models.
In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview
.net/forum?id=fpoAYV6Wsk.",2024,"https://openreview
.net/forum?id=fpoAYV6Wsk."
"P. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_fil
es/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf.",2019,"https://proceedings.neurips.cc/paper_fil
es/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf."
"T. Mickus, D. Paperno, and M. Constant. How to dissect a Muppet: The structure of transformer embedding
spaces. Transactions of the Association for Computational Linguistics, 10:981–996, 2022. doi: 10.1162/ta
cl_a_00501. URL https://aclanthology.org/2022.tacl-1.57.",2022,https://aclanthology.org/2022.tacl-1.57.
"V. Miglani, A. Yang, A. Markosyan, D. Garcia-Olano, and N. Kokhlikyan. Using captum to explain generative
language models. In L. Tan, D. Milajevs, G. Chauhan, J. Gwinnup, and E. Rippeth (eds.), Proceedings of
the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pp. 165–173,
Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.nlposs-1.19.
URL https://aclanthology.org/2023.nlposs-1.19.",2023,https://aclanthology.org/2023.nlposs-1.19.
"T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and
phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger
(eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL
https://proceedings.neurips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923c
e901b-Abstract.html.",2013,"https://proceedings.neurips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923c
e901b-Abstract.html."
"B. Millidge and S. Black. The singular value decompositions of transformer weight matrices are highly interpretable. AI Alignment Forum, 2022. URL https://www.alignmentforum.org/posts/mkbGjzxD8d8Xq
KHzA/the-singular-value-decompositions-of-transformer-weight.",2022,"https://www.alignmentforum.org/posts/mkbGjzxD8d8Xq
KHzA/the-singular-value-decompositions-of-transformer-weight."
"B. Millidge and E. Winsor. Basic facts about language model internals. AI Alignment Forum, 2023. URL
https://www.alignmentforum.org/posts/PDLfpRwSynu73mxGw/basic-facts-about-language-mod
el-internals-1.",2023,"https://www.alignmentforum.org/posts/PDLfpRwSynu73mxGw/basic-facts-about-language-mod
el-internals-1."
"S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao. Large language models:
A survey, 2024. URL https://arxiv.org/abs/2402.06196.",2024,https://arxiv.org/abs/2402.06196.
"E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning. Fast model editing at scale. In International
Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=0DcZxeWf
OPt.",,"https://openreview.net/forum?id=0DcZxeWf
OPt."
"E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn. Memory-based model editing at scale. In
K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato (eds.), Proceedings of the 39th
International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,
pp. 15817–15831. PMLR, 17–23 Jul 2022b. URL https://proceedings.mlr.press/v162/mitchell2
2a.html.",,"https://proceedings.mlr.press/v162/mitchell2
2a.html."
"A. Modarressi, M. Fayyaz, Y. Yaghoobzadeh, and M. T. Pilehvar. GlobEnc: Quantifying global token attribution by incorporating the whole encoder layer in transformers. In M. Carpuat, M.-C. de Marneffe, and
I. V. Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 258–271, Seattle, United States,
July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.19. URL
https://aclanthology.org/2022.naacl-main.19.",2022,https://aclanthology.org/2022.naacl-main.19.
"A. Modarressi, M. Fayyaz, E. Aghazadeh, Y. Yaghoobzadeh, and M. T. Pilehvar. DecompX: Explaining transformers decisions by propagating token decomposition. In A. Rogers, J. Boyd-Graber, and N. Okazaki
(eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pp. 2649–2664, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:
10.18653/v1/2023.acl-long.149. URL https://aclanthology.org/2023.acl-long.149.",2023,https://aclanthology.org/2023.acl-long.149.
"H. Mohebbi, W. Zuidema, G. Chrupała, and A. Alishahi. Quantifying context mixing in transformers. In
A. Vlachos and I. Augenstein (eds.), Proceedings of the 17th Conference of the European Chapter of the
Association for Computational Linguistics, pp. 3378–3400, Dubrovnik, Croatia, May 2023. Association for
Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.245. URL https://aclanthology.org/2
023.eacl-main.245.",2023,"https://aclanthology.org/2
023.eacl-main.245."
"R. Molina. Traveling words: A geometric interpretation of transformers. Arxiv, 2023. URL https://arxiv.
org/abs/2309.07315.",2023,"https://arxiv.
org/abs/2309.07315."
"G. Monea, M. Peyrard, M. Josifoski, V. Chaudhary, J. Eisner, E. Kıcıman, H. Palangi, B. Patra, and R. West.
A glitch in the matrix? locating and detecting language model grounding with fakepedia, 2024. URL
https://arxiv.org/abs/2312.02073.",2024,https://arxiv.org/abs/2312.02073.
"D. Mossing, S. Bills, H. Tillman, T. Dupré la Tour, N. Cammarata, L. Gao, J. Achiam, C. Yeh, J. Leike, J. Wu,
and W. Saunders. Transformer debugger. https://github.com/openai/transformer-debugger, 2024.",2024,
"N. Nanda. Induction mosaic. Neel Nanda Blog, 2022a. URL https://neelnanda.io/mosaic.",,https://neelnanda.io/mosaic.
"N. Nanda. Neuroscope: A website for mechanistic interpretability of language models. Website, 2022b. URL
https://neuroscope.io/.",,https://neuroscope.io/.
"N. Nanda. Attribution patching: Activation patching at industrial scale. https://www.neelnanda.io/mecha
nistic-interpretability/attribution-patching, 2023.",2023,
"N. Nanda and J. Bloom. Transformerlens. Github Repository, 2022. URL https://github.com/neelnanda
-io/TransformerLens.",2022,"https://github.com/neelnanda
-io/TransformerLens."
"N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. Progress measures for grokking via mechanistic
interpretability. In The Eleventh International Conference on Learning Representations, 2023a. URL https:
//openreview.net/forum?id=9XFSbDPmdW.",,"https:
//openreview.net/forum?id=9XFSbDPmdW."
"N. Nanda, A. Lee, and M. Wattenberg. Emergent linear representations in world models of self-supervised
sequence models. In Y. Belinkov, S. Hao, J. Jumelet, N. Kim, A. McCarthy, and H. Mohebbi (eds.), Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp.
16–30, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.bla
ckboxnlp-1.2. URL https://aclanthology.org/2023.blackboxnlp-1.2.",2023,https://aclanthology.org/2023.blackboxnlp-1.2.
"N. Nanda, S. Rajamanoharan, J. Kramár, and R. Shah. Fact finding: Attempting to reverse-engineer factual
recall on the neuron level. AI Alignment Forum, 2023c. URL https://www.alignmentforum.org/posts
/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall.",,"https://www.alignmentforum.org/posts
/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall."
"C. Neo, S. B. Cohen, and F. Barez. Interpreting context look-ups in transformers: Investigating attention-mlp
interactions, 2024. URL https://arxiv.org/abs/2402.15055.",2024,https://arxiv.org/abs/2402.15055.
"A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune. Synthesizing the preferred inputs for neurons in
neural networks via deep generator networks. In Proceedings of the 30th International Conference on Neural
Information Processing Systems, NIPS’16, pp. 3395–3403, Red Hook, NY, USA, 2016. Curran Associates
Inc. ISBN 9781510838819.",2016,
"R. Nogueira, Z. Jiang, and J. Lin. Investigating the limitations of transformers with simple arithmetic tasks,
2021.",2021,
"nostalgebraist. Interpreting GPT: the logit lens. AI Alignment Forum, 2020. URL https://www.alignmentf
orum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.",2020,"https://www.alignmentf
orum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens."
"B.-D. Oh and W. Schuler. Token-wise decomposition of autoregressive language model hidden states for
analyzing model predictions. In A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.), Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10105–
10117, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl
-long.562. URL https://aclanthology.org/2023.acl-long.562.",2023,https://aclanthology.org/2023.acl-long.562.
"C. Olah. Mechanistic interpretability, variables, and the importance of interpretable bases. Transformer Circuits
Thread, 2022. URL https://transformer-circuits.pub/2022/mech-interp-essay.",2022,https://transformer-circuits.pub/2022/mech-interp-essay.
"C. Olah. Distributed representations: Composition & superposition. Transformer Circuits Thread, 2023. URL
https://transformer-circuits.pub/2023/superposition-composition/index.html.",2023,https://transformer-circuits.pub/2023/superposition-composition/index.html.
"C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. An overview of early vision in
inceptionv1. Distill, 2020a. doi: 10.23915/distill.00024.002. https://distill.pub/2020/circuits/early-vision.",2020,
"C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom in: An introduction to circuits.
Distill, 2020b. doi: 10.23915/distill.00024.001. URL https://distill.pub/2020/circuits/zoom-in.",2020,https://distill.pub/2020/circuits/zoom-in.
"B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by v1?
Vision Research, 37(23):3311–3325, 1997. ISSN 0042-6989. doi: https://doi.org/10.1016/S0042-6989(97
)00169-7. URL https://www.sciencedirect.com/science/article/pii/S0042698997001697.",1997,https://www.sciencedirect.com/science/article/pii/S0042698997001697.
"C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen,
T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion,
L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context
learning and induction heads. Transformer Circuits Thread, 2022. URL https://transformer-circuit
s.pub/2022/in-context-learning-and-induction-heads/index.html.",2022,"https://transformer-circuit
s.pub/2022/in-context-learning-and-induction-heads/index.html."
"F. Ortu, Z. Jin, D. Doimo, M. Sachan, A. Cazzaniga, and B. Schölkopf. Competition of mechanisms: Tracing
how language models handle facts and counterfactuals. Computing Research Repository, arXiv:2402.11655,
2024. URL https://arxiv.org/abs/2402.11655.",2024,https://arxiv.org/abs/2402.11655.
"G. PAIR Team. Saliency: Framework-agnostic implementation for state-of-the-art saliency methods, 2023.
URL https://github.com/PAIR-code/saliency.",2023,https://github.com/PAIR-code/saliency.
"K. Pal, J. Sun, A. Yuan, B. Wallace, and D. Bau. Future lens: Anticipating subsequent tokens from a single
hidden state. In J. Jiang, D. Reitter, and S. Deng (eds.), Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pp. 548–560, Singapore, December 2023. Association for
Computational Linguistics. doi: 10.18653/v1/2023.conll-1.37. URL https://aclanthology.org/2023.
conll-1.37.",2023,"https://aclanthology.org/2023.
conll-1.37."
"L. Parcalabescu and A. Frank. On measuring faithfulness or self-consistency of natural language explanations,
2023.",2023,
"K. Park, Y. J. Choe, and V. Veitch. The linear representation hypothesis and the geometry of large language
models. Arxiv, 2023a. URL https://arxiv.org/abs/2311.03658.",,https://arxiv.org/abs/2311.03658.
"S. M. Park, K. Georgiev, A. Ilyas, G. Leclerc, and A. M ˛adry. Trak: attributing model behavior at scale. In
Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023b.",,
"G. Paulo, T. Marshall, and N. Belrose. Does transformer interpretability transfer to rnns?, 2024.",2024,
"J. Pearl. Direct and indirect effects. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial
Intelligence, UAI’01, pp. 411–420, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN
1558608001.",2001,
"J. Pearl. Causality. Cambridge University Press, 2 edition, 2009. doi: 10.1017/CBO9780511803161.",2009,
"M. E. Peters, M. Neumann, L. Zettlemoyer, and W.-t. Yih. Dissecting contextual word embeddings: Architecture and representation. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii (eds.), Proceedings of the
2018 Conference on Empirical Methods in Natural Language Processing, pp. 1499–1509, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1179.
URL https://aclanthology.org/D18-1179.",2018,https://aclanthology.org/D18-1179.
"P. Pezeshkpour, S. Jain, S. Singh, and B. Wallace. Combining feature and instance attribution to detect artifacts.
In S. Muresan, P. Nakov, and A. Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 1934–1946, Dublin, Ireland, May 2022. Association for Computational Linguistics.
doi: 10.18653/v1/2022.findings-acl.153. URL https://aclanthology.org/2022.findings-acl.153.",2022,https://aclanthology.org/2022.findings-acl.153.
"C. Pierse. Transformers Interpret, February 2021. URL https://github.com/cdpierse/transformers-i
nterpret.",2021,"https://github.com/cdpierse/transformers-i
nterpret."
"T. Pimentel, J. Valvoda, R. H. Maudslay, R. Zmigrod, A. Williams, and R. Cotterell. Information-theoretic
probing for linguistic structure. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault (eds.), Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4609–4622, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.420. URL https:
//aclanthology.org/2020.acl-main.420.",2020,"https:
//aclanthology.org/2020.acl-main.420."
"A. Power, Y. Burda, H. Edwards, I. Babuschkin, and V. Misra. Grokking: Generalization beyond overfitting on
small algorithmic datasets, 2022.",2022,
"N. Prakash, T. R. Shaham, T. Haklay, Y. Belinkov, and D. Bau. Fine-tuning enhances existing mechanisms: A
case study on entity tracking. In The Twelfth International Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=8sKcAWOf2D.",2024,https://openreview.net/forum?id=8sKcAWOf2D.
"G. Puccetti, A. Rogers, A. Drozd, and F. Dell’Orletta. Outlier dimensions that disrupt transformers are driven by
frequency. In Y. Goldberg, Z. Kozareva, and Y. Zhang (eds.), Findings of the Association for Computational
Linguistics: EMNLP 2022, pp. 1286–1304, Abu Dhabi, United Arab Emirates, December 2022. Association
for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.93. URL https://aclanthology
.org/2022.findings-emnlp.93.",2022,"https://aclanthology
.org/2022.findings-emnlp.93."
"J. Qi, R. Fernández, and A. Bisazza. Cross-lingual consistency of factual knowledge in multilingual language
models. In H. Bouamor, J. Pino, and K. Bali (eds.), Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, pp. 10650–10666, Singapore, December 2023. Association for
Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.658. URL https://aclanthology.org
/2023.emnlp-main.658.",2023,"https://aclanthology.org
/2023.emnlp-main.658."
"L. Quirke, L. Heindrich, W. Gurnee, and N. Nanda. Training dynamics of contextual n-grams in language
models, 2023. URL https://arxiv.org/abs/2311.00863.",2023,https://arxiv.org/abs/2311.00863.
"A. Radford, R. Jozefowicz, and I. Sutskever. Learning to generate reviews and discovering sentiment. Arxiv,
2017. URL https://arxiv.org/abs/1704.01444.",2017,https://arxiv.org/abs/1704.01444.
"A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative
pre-training. OpenAI Blog, 2018. URL https://openai.com/research/language-unsupervised.",2018,https://openai.com/research/language-unsupervised.
"A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised
multitask learners. OpenAI Blog, 2019. URL https://d4mucfpksywv.cloudfront.net/better-langu
age-models/language_models_are_unsupervised_multitask_learners.pdf.",2019,"https://d4mucfpksywv.cloudfront.net/better-langu
age-models/language_models_are_unsupervised_multitask_learners.pdf."
"A. Raganato and J. Tiedemann. An analysis of encoder representations in transformer-based machine translation. In T. Linzen, G. Chrupała, and A. Alishahi (eds.), Proceedings of the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 287–297, Brussels, Belgium,
November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18- 5431. URL
https://aclanthology.org/W18-5431.",2018,https://aclanthology.org/W18-5431.
"S. Rajamanoharan. Progress update 1 from the gdm mech interp team. improving ghost grads. AI Alignment
Forum, 2024. URL https://www.alignmentforum.org/posts/C5KAZQib3bzzpeyrg/progress-updat
e-1-from-the-gdm-mech-interp-team-full-update.",2024,"https://www.alignmentforum.org/posts/C5KAZQib3bzzpeyrg/progress-updat
e-1-from-the-gdm-mech-interp-team-full-update."
"S. Rajamanoharan, A. Conmy, L. Smith, T. Lieberum, V. Varma, J. Kramár, R. Shah, and N. Nanda. Improving
dictionary learning with gated sparse autoencoders. ArXiv, 2024.",2024,
"S. Ravfogel, Y. Elazar, H. Gonen, M. Twiton, and Y. Goldberg. Null it out: Guarding protected attributes
by iterative nullspace projection. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault (eds.), Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7237–7256, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.647. URL https:
//aclanthology.org/2020.acl-main.647.",2020,"https:
//aclanthology.org/2020.acl-main.647."
"S. Ravfogel, M. Twiton, Y. Goldberg, and R. D. Cotterell. Linear adversarial concept erasure. In K. Chaudhuri,
S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato (eds.), Proceedings of the 39th International
Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 18400–
18421. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ravfogel22a.html.",2022,https://proceedings.mlr.press/v162/ravfogel22a.html.
"M. T. Ribeiro, S. Singh, and C. Guestrin. ""why should I trust you?"": Explaining the predictions of any classifier.
In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 1135–1144, 2016.",2016,
"A. Rogers, O. Kovaleva, and A. Rumshisky. A Primer in BERTology: What We Know About How BERT
Works. Transactions of the Association for Computational Linguistics, 8:842–866, 01 2021. ISSN 2307-
387X. doi: 10.1162/tacl_a_00349. URL https://doi.org/10.1162/tacl_a_00349.",2021,https://doi.org/10.1162/tacl_a_00349.
"W. Rudman, C. Chen, and C. Eickhoff. Outlier dimensions encode task specific knowledge. In H. Bouamor,
J. Pino, and K. Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing, pp. 14596–14605, Singapore, December 2023. Association for Computational Linguistics. doi:
10.18653/v1/2023.emnlp-main.901. URL https://aclanthology.org/2023.emnlp-main.901.",2023,https://aclanthology.org/2023.emnlp-main.901.
"C. Rushing and N. Nanda. Explorations of self-repair in language models, 2024. URL https://arxiv.org/
abs/2402.15390.",2024,"https://arxiv.org/
abs/2402.15390."
"T. Räuker, A. Ho, S. Casper, and D. Hadfield-Menell. Toward transparent ai: A survey on interpreting the inner
structures of deep neural networks. Arxiv, 2023. URL https://arxiv.org/abs/2207.13243.",2023,https://arxiv.org/abs/2207.13243.
"M. Sakarvadia, A. Khan, A. Ajith, D. Grzenda, N. Hudson, A. Bauer, K. Chard, and I. Foster. Attention lens:
A tool for mechanistically interpreting the attention head information retrieval mechanism, 2023.",2023,
"S. Sanyal and X. Ren. Discretized integrated gradients for explaining language models. In M.-F. Moens,
X. Huang, L. Specia, and S. W.-t. Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, pp. 10285–10299, Online and Punta Cana, Dominican Republic, November
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.805. URL https:
//aclanthology.org/2021.emnlp-main.805.",2021,"https:
//aclanthology.org/2021.emnlp-main.805."
"G. Sarti, N. Feldhus, L. Sickert, O. van der Wal, M. Nissim, and A. Bisazza. Inseq: An interpretability
toolkit for sequence generation models. In Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 3: System Demonstrations), pp. 421–435, Toronto, Canada, July
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-demo.40. URL https:
//aclanthology.org/2023.acl-demo.40.",2023,"https:
//aclanthology.org/2023.acl-demo.40."
"G. Sarti, G. Chrupała, M. Nissim, and A. Bisazza. Quantifying the plausibility of context reliance in neural
machine translation. In The Twelfth International Conference on Learning Representations (ICLR 2024),
Vienna, Austria, May 2024. OpenReview. URL https://openreview.net/forum?id=XTHfNGI3zT.",2024,https://openreview.net/forum?id=XTHfNGI3zT.
"T. R. Shaham, S. Schwettmann, F. Wang, A. Rajaram, E. Hernandez, J. Andreas, and A. Torralba. A multimodal
automated interpretability agent. Arxiv, 2024. URL https://arxiv.org/abs/2404.14394.",2024,https://arxiv.org/abs/2404.14394.
"L. S. Shapley. A value for n-person games. In H. W. Kuhn and A. W. Tucker (eds.), Contributions to the Theory
of Games II, pp. 307–317. Princeton University Press, Princeton, 1953.",1953,
"L. Sharkey, D. Braun, and B. Millidge. Taking features out of superposition with sparse autoencoders. AI
Alignment Forum, 2022. URL https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/inter
im-research-report-taking-features-out-of-superposition.",2022,"https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/inter
im-research-report-taking-features-out-of-superposition."
"A. S. Sharma, D. Atkinson, and D. Bau. Locating and editing factual associations in mamba, 2024a.",,
"P. Sharma, J. T. Ash, and D. Misra. The truth is in there: Improving reasoning with layer-selective rank
reduction. In The Twelfth International Conference on Learning Representations, 2024b. URL https:
//openreview.net/forum?id=ozX92bu8VA.",,"https:
//openreview.net/forum?id=ozX92bu8VA."
"N. Shazeer. Glu variants improve transformer. ArXiv, 2020.",2020,
"A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,
pp. 3145–3153. JMLR.org, 2017.",2017,
"A. Shrikumar, J. Su, and A. Kundaje. Computationally efficient measures of internal neuron importance. ArXiv,
abs/1807.09946, 2018. URL https://api.semanticscholar.org/CorpusID:50787065.",2018,https://api.semanticscholar.org/CorpusID:50787065.
"N. Y. Siegel, O.-M. Camburu, N. Heess, and M. Perez-Ortiz. The probabilities also matter: A more faithful
metric for faithfulness of free-text explanations in large language models, 2024.",2024,
"K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In The Second International Conference on Learning Representations,
2014. URL http://arxiv.org/abs/1312.6034.",2014,http://arxiv.org/abs/1312.6034.
"A. K. Singh, T. Moskovitz, F. Hill, S. C. Y. Chan, and A. M. Saxe. What needs to go right for an induction
head? a mechanistic study of in-context learning circuits and their formation, 2024a.",,
"C. Singh, J. P. Inala, M. Galley, R. Caruana, and J. Gao. Rethinking interpretability in the era of large language
models. ArXiv, abs/2402.01761, 2024b. URL https://api.semanticscholar.org/CorpusID:26741253
0.",,"https://api.semanticscholar.org/CorpusID:26741253
0."
"S. Singh, S. Ravfogel, J. Herzig, R. Aharoni, R. Cotterell, and P. Kumaraguru. Mimic: Minimally modified
counterfactuals in the representation space. Arxiv, 2024c. URL https://arxiv.org/abs/2402.09631.",,https://arxiv.org/abs/2402.09631.
"L. Sixt, M. Granz, and T. Landgraf. When explanations lie: Why many modified BP attributions fail. In
H. D. III and A. Singh (eds.), Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pp. 9046–9057. PMLR, 13–18 Jul 2020. URL
https://proceedings.mlr.press/v119/sixt20a.html.",2020,https://proceedings.mlr.press/v119/sixt20a.html.
"D. Smilkov, N. Thorat, B. Kim, F. Viégas, and M. Wattenberg. Smoothgrad: removing noise by adding noise,
2017.",2017,
"P. Smolensky. Neural and conceptual interpretation of PDP models, pp. 390–431. MIT Press, Cambridge,
MA, USA, 1986. ISBN 0262631105.",1986,
"N. Stoehr, M. Gordon, C. Zhang, and O. Lewis. Localizing paragraph memorization in language models, 2024.
URL https://arxiv.org/abs/2403.19851.",2024,https://arxiv.org/abs/2403.19851.
"A. Stolfo, Y. Belinkov, and M. Sachan. A mechanistic interpretation of arithmetic reasoning in language
models using causal mediation analysis. In H. Bouamor, J. Pino, and K. Bali (eds.), Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing, pp. 7035–7052, Singapore, December
2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.435. URL https:
//aclanthology.org/2023.emnlp-main.435.",2023,"https:
//aclanthology.org/2023.emnlp-main.435."
"A. Stolfo, Y. Belinkov, and M. Sachan. Understanding arithmetic reasoning in language models using causal
mediation analysis. Arxiv, 2023b. URL https://arxiv.org/abs/2305.15054.",,https://arxiv.org/abs/2305.15054.
"X. Suau, L. Zappella, and N. Apostoloff. Finding experts in transformer models, 2020.",2020,
"X. Suau, L. Zappella, and N. Apostoloff. Self-conditioning pre-trained language models. International Conference on Machine Learning, 2022. URL https://proceedings.mlr.press/v162/cuadros22a/cuad
ros22a.pdf.",2022,"https://proceedings.mlr.press/v162/cuadros22a/cuad
ros22a.pdf."
"M. Sun, X. Chen, J. Z. Kolter, and Z. Liu. Massive activations in large language models, 2024. URL https:
//arxiv.org/abs/2402.17762.",2024,"https:
//arxiv.org/abs/2402.17762."
"M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th
International Conference on Machine Learning, volume 70, pp. 3319–3328. JMLR.org, 2017.",2017,
"A. Syed, C. Rager, and A. Conmy. Attribution patching outperforms automated circuit discovery. Arxiv, 2023.
URL https://arxiv.org/abs/2310.10348.",2023,https://arxiv.org/abs/2310.10348.
"S. Takase, S. Kiyono, S. Kobayashi, and J. Suzuki. B2T connection: Serving stability and performance in deep
transformers. In A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078–3095, Toronto, Canada, July 2023. Association for Computational
Linguistics. doi: 10.18653/v1/2023.findings-acl.192. URL https://aclanthology.org/2023.findings
-acl.192.",2023,"https://aclanthology.org/2023.findings
-acl.192."
"T. Tang, W. Luo, H. Huang, D. Zhang, X. Wang, X. Zhao, F. Wei, and J.-R. Wen. Language-specific neurons:
The key to multilingual capabilities in large language models, 2024. URL https://arxiv.org/abs/2402
.16438.",2024,"https://arxiv.org/abs/2402
.16438."
"D. A. Tarzanagh, Y. Li, C. Thrampoulidis, and S. Oymak. Transformers as support vector machines, 2024.",2024,
"A. Templeton, T. Conerly, J. Marcus, T. Henighan, A. Golubeva, and T. Bricken. Circuits updates - february
2024. update on dictionary learning improvements. Transformer Circuits Thread, 2024. URL https:
//transformer-circuits.pub/2024/feb-update/index.html.",2024,"https:
//transformer-circuits.pub/2024/feb-update/index.html."
"I. Tenney, D. Das, and E. Pavlick. BERT rediscovers the classical NLP pipeline. In A. Korhonen, D. Traum, and
L. Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
pp. 4593–4601, Florence, Italy, July 2019a. Association for Computational Linguistics. doi: 10.18653/v1/
P19-1452. URL https://aclanthology.org/P19-1452.",,https://aclanthology.org/P19-1452.
"I. Tenney, P. Xia, B. Chen, A. Wang, A. Poliak, R. T. McCoy, N. Kim, B. V. Durme, S. Bowman, D. Das,
and E. Pavlick. What do you learn from context? probing for sentence structure in contextualized word
representations. In International Conference on Learning Representations, 2019b. URL https://openre
view.net/forum?id=SJzSgnRcKX.",,"https://openre
view.net/forum?id=SJzSgnRcKX."
"I. Tenney, J. Wexler, J. Bastings, T. Bolukbasi, A. Coenen, S. Gehrmann, E. Jiang, M. Pushkarna, C. Radebaugh,
E. Reif, and A. Yuan. The language interpretability tool: Extensible, interactive visualizations and analysis
for NLP models. In Q. Liu and D. Schlangen (eds.), Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System Demonstrations, pp. 107–118, Online, October 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.15. URL https:
//aclanthology.org/2020.emnlp-demos.15.",2020,"https:
//aclanthology.org/2020.emnlp-demos.15."
"I. Tenney, R. Mullins, B. Du, S. Pandya, M. Kahng, and L. Dixon. Interactive prompt debugging with sequence
salience. Arxiv, 2024. URL https://arxiv.org/abs/2404.07498.",2024,https://arxiv.org/abs/2404.07498.
"Y. Tian, Y. Wang, B. Chen, and S. Du. Scan and snap: Understanding training dynamics and token composition
in 1-layer transformer, 2023.",2023,
"Y. Tian, Y. Wang, Z. Zhang, B. Chen, and S. S. Du. JoMA: Demystifying multilayer transformers via joint
dynamics of MLP and attention. In The Twelfth International Conference on Learning Representations,
2024. URL https://openreview.net/forum?id=LbJqRGNYCf.",2024,https://openreview.net/forum?id=LbJqRGNYCf.
"R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series
B (Methodological), 58(1):267–288, 1996. doi: https://doi.org/10.1111/j.2517-6161.1996.tb02080.x. URL
https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x.",1996,https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x.
"C. Tigges, O. J. Hollinsworth, A. Geiger, and N. Nanda. Linear representations of sentiment in large language
models. Arxiv, 2023. URL https://arxiv.org/abs/2310.15154.",2023,https://arxiv.org/abs/2310.15154.
"W. Timkey and M. van Schijndel. All bark and no bite: Rogue dimensions in transformer language models
obscure representational quality. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih (eds.), Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 4527–4546, Online
and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.emnlp-main.372. URL https://aclanthology.org/2021.emnlp-main.372.",2021,https://aclanthology.org/2021.emnlp-main.372.
"E. Todd, M. Li, A. S. Sharma, A. Mueller, B. C. Wallace, and D. Bau. LLMs represent contextual tasks as
compact function vectors. In The Twelfth International Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=AwyxtyMwaG.",2024,https://openreview.net/forum?id=AwyxtyMwaG.
"H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,
S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu,
B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez,
M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu,
Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta,
K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams,
J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,
S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. Arxiv, 2023. URL
https://arxiv.org/abs/2307.09288.",2023,https://arxiv.org/abs/2307.09288.
"I. Tufanov, K. Hambardzumyan, J. Ferrando, and E. Voita. Lm transparency tool: Interactive tool for analyzing
transformer language models. Arxiv, 2024. URL https://arxiv.org/abs/2404.07004.",2024,https://arxiv.org/abs/2404.07004.
"A. M. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, and M. MacDiarmid. Activation addition: Steering
language models without optimization, 2023.",2023,
"M. Turpin, J. Michael, E. Perez, and S. Bowman. Language models don’t always say what they think:
Unfaithful explanations in chain-of-thought prompting. ArXiv, abs/2305.04388, 2023. URL https:
//api.semanticscholar.org/CorpusID:258556812.",2023,"https:
//api.semanticscholar.org/CorpusID:258556812."
"A. Variengien. Some common confusion about induction heads. LessWrong, 2023. URL https://www.less
wrong.com/posts/nJqftacoQGKurJ6fv/some-common-confusion-about-induction-heads.",2023,"https://www.less
wrong.com/posts/nJqftacoQGKurJ6fv/some-common-confusion-about-induction-heads."
"A. Variengien and E. Winsor. Look before you leap: A universal emergent decomposition of retrieval tasks in
language models, 2023. URL https://arxiv.org/abs/2312.10091.",2023,https://arxiv.org/abs/2312.10091.
"V. Varma, R. Shah, Z. Kenton, J. Kram’ar, and R. Kumar. Explaining grokking through circuit efficiency.
ArXiv, abs/2309.02390, 2023. URL https://api.semanticscholar.org/CorpusID:261557247.",2023,https://api.semanticscholar.org/CorpusID:261557247.
"N. Varshney, W. Yao, H. Zhang, J. Chen, and D. Yu. A stitch in time saves nine: Detecting and mitigating
hallucinations of llms by validating low-confidence generation, 2023. URL https://arxiv.org/abs/23
07.03987.",2023,"https://arxiv.org/abs/23
07.03987."
"H. Vasconcelos, M. Jörke, M. Grunde-McLaughlin, T. Gerstenberg, M. S. Bernstein, and R. Krishna. Explanations can reduce overreliance on ai systems during decision-making. Proc. ACM Hum.-Comput. Interact., 7
(CSCW1), apr 2023. doi: 10.1145/3579605. URL https://doi.org/10.1145/3579605.",2023,https://doi.org/10.1145/3579605.
"A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin.
Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates,
Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c
4a845aa-Paper.pdf.",2017,"https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c
4a845aa-Paper.pdf."
"A. Veit, M. Wilber, and S. Belongie. Residual networks behave like ensembles of relatively shallow networks.
In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16,
pp. 550–558, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.",2016,
"J. Vig. A multiscale visualization of attention in the transformer model. In M. R. Costa-jussà and E. Alfonseca
(eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System
Demonstrations, pp. 37–42, Florence, Italy, July 2019. Association for Computational Linguistics. doi:
10.18653/v1/P19-3007. URL https://aclanthology.org/P19-3007.",2019,https://aclanthology.org/P19-3007.
"J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer, and S. Shieber. Investigating gender bias in
language models using causal mediation analysis. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 12388–12401. Curran
Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/92650b2e9221771
5fe312e6fa7b90d82-Abstract.html.",2020,"https://proceedings.neurips.cc/paper/2020/hash/92650b2e9221771
5fe312e6fa7b90d82-Abstract.html."
"E. Voita and I. Titov. Information-theoretic probing with minimum description length. In B. Webber, T. Cohn,
Y. He, and Y. Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 183–196, Online, November 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.emnlp-main.14. URL https://aclanthology.org/2020.emnlp-main.14.",2020,https://aclanthology.org/2020.emnlp-main.14.
"E. Voita, R. Sennrich, and I. Titov. The bottom-up evolution of representations in the transformer: A study
with machine translation and language modeling objectives. In K. Inui, J. Jiang, V. Ng, and X. Wan (eds.),
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4396–4406, Hong
Kong, China, November 2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1448.
URL https://aclanthology.org/D19-1448.",2019,https://aclanthology.org/D19-1448.
"E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing multi-head self-attention: Specialized
heads do the heavy lifting, the rest can be pruned. In A. Korhonen, D. Traum, and L. Màrquez (eds.),
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5797–5808,
Florence, Italy, July 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1580. URL
https://aclanthology.org/P19-1580.",,https://aclanthology.org/P19-1580.
"E. Voita, R. Sennrich, and I. Titov. Analyzing the source and target contributions to predictions in neural
machine translation. In C. Zong, F. Xia, W. Li, and R. Navigli (eds.), Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers), pp. 1126–1140, Online, August 2021. Association
for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.91. URL https://aclanthology.org/2
021.acl-long.91.",2021,"https://aclanthology.org/2
021.acl-long.91."
"E. Voita, J. Ferrando, and C. Nalmpantis. Neurons in large language models: Dead, n-gram, positional. Arxiv,
2023. URL https://arxiv.org/abs/2309.04827.",2023,https://arxiv.org/abs/2309.04827.
"J. Von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov.
Transformers learn in-context by gradient descent. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt,
S. Sabato, and J. Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning,
volume 202 of Proceedings of Machine Learning Research, pp. 35151–35174. PMLR, 23–29 Jul 2023.
URL https://proceedings.mlr.press/v202/von-oswald23a.html.",2023,https://proceedings.mlr.press/v202/von-oswald23a.html.
"K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit
for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning
Representations, 2023a. URL https://openreview.net/forum?id=NpsVSN6o4ul.",,https://openreview.net/forum?id=NpsVSN6o4ul.
"Q. Wang, T. Anikina, N. Feldhus, J. van Genabith, L. Hennig, and S. Möller. Llmcheckup: Conversational
examination of large language models via interpretability tools, 2024.",2024,
"S. Wang, Y. Zhu, H. Liu, Z. Zheng, C. Chen, and J. Li. Knowledge editing for large language models: A survey.
ArXiv, abs/2310.16218, 2023b. URL https://api.semanticscholar.org/CorpusID:264487359.",,https://api.semanticscholar.org/CorpusID:264487359.
"X. Wang, K. Wen, Z. Zhang, L. Hou, Z. Liu, and J. Li. Finding skill neurons in pre-trained transformer-based
language models. In Y. Goldberg, Z. Kozareva, and Y. Zhang (eds.), Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing, pp. 11132–11152, Abu Dhabi, United Arab Emirates,
December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.765. URL
https://aclanthology.org/2022.emnlp-main.765.",2022,https://aclanthology.org/2022.emnlp-main.765.
"D. Wei, R. Nair, A. Dhurandhar, K. R. Varshney, E. Daly, and M. Singh. On the safety of interpretable machine
learning: A maximum deviation approach. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,
and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 9866–9880. Curran
Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/402
e12102d6ec3ea3df40ce1b23d423a-Paper-Conference.pdf.",2022,"https://proceedings.neurips.cc/paper_files/paper/2022/file/402
e12102d6ec3ea3df40ce1b23d423a-Paper-Conference.pdf."
"G. Weiss, Y. Goldberg, and E. Yahav. Thinking like transformers. In M. Meila and T. Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning
Research, pp. 11080–11090. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/wei
ss21a.html.",2021,"https://proceedings.mlr.press/v139/wei
ss21a.html."
"K. Wen, Y. Li, B. Liu, and A. Risteski. Transformers are uninterpretable with myopic methods: a case study
with bounded dyck grammars. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine
(eds.), Advances in Neural Information Processing Systems, volume 36, pp. 38723–38766. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/79ba1b8
27d3fc58e129d1cbfc8ff69f2-Paper-Conference.pdf.",2023,"https://proceedings.neurips.cc/paper_files/paper/2023/file/79ba1b8
27d3fc58e129d1cbfc8ff69f2-Paper-Conference.pdf."
"N. Wichers, C. Denison, and A. Beirami. Gradient-based language model red teaming, 2024.",2024,
"T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,
J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame,
Q. Lhoest, and A. Rush. Transformers: State-of-the-art natural language processing. In Q. Liu and
D. Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6.",2020,https://aclanthology.org/2020.emnlp-demos.6.
"B. Wright and L. Sharkey. Addressing feature suppression in saes. AI ALIGNMENT FORUM, 2024. URL
https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression
-in-saes.",2024,"https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression
-in-saes."
"W. Wu, Y. Wang, G. Xiao, H. Peng, and Y. Fu. Retrieval head mechanistically explains long-context factuality.
Arxiv, 2024a. URL https://arxiv.org/abs/2404.15574.",,https://arxiv.org/abs/2404.15574.
"Z. Wu, K. D’Oosterlinck, A. Geiger, A. Zur, and C. Potts. Causal proxy models for concept-based model explanations. In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org,
2023a.",,
"Z. Wu, A. Geiger, T. Icard, C. Potts, and N. Goodman. Interpretability at scale: Identifying causal mechanisms
in alpaca. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in
Neural Information Processing Systems, volume 36, pp. 78205–78226. Curran Associates, Inc., 2023b. URL
https://proceedings.neurips.cc/paper_files/paper/2023/file/f6a8b109d4d4fd64c75e94aaf85
d9697-Paper-Conference.pdf.",2023,"https://proceedings.neurips.cc/paper_files/paper/2023/file/f6a8b109d4d4fd64c75e94aaf85
d9697-Paper-Conference.pdf."
"Z. Wu, A. Arora, Z. Wang, A. Geiger, D. Jurafsky, C. D. Manning, and C. Potts. Reft: Representation finetuning
for language models, 2024b.",,
"Z. Wu, A. Geiger, A. Arora, J. Huang, Z. Wang, N. D. Goodman, C. D. Manning, and C. Potts. pyvene: A
library for understanding and improving pytorch models via interventions, 2024c.",,
"Z. Wu, A. Geiger, J. Huang, A. Arora, T. Icard, C. Potts, and N. D. Goodman. A reply to makelov et al. (2023)’s
""interpretability illusion"" arguments, 2024d.",2023,
"G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks.
Arxiv, 2023. URL https://arxiv.org/abs/2309.17453.",2023,https://arxiv.org/abs/2309.17453.
"S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian
inference. In International Conference on Learning Representations, 2022. URL https://openreview.n
et/forum?id=RdJVFCHjUMI.",2022,"https://openreview.n
et/forum?id=RdJVFCHjUMI."
"R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T.-Y. Liu. On
layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on
Machine Learning. JMLR.org, 2020.",2020,
"S. Yang, S. Huang, W. Zou, J. Zhang, X. Dai, and J. Chen. Local interpretation of transformer based on linear
decomposition. In A. Rogers, J. Boyd-Graber, and N. Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10270–10287, Toronto,
Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.572. URL
https://aclanthology.org/2023.acl-long.572.",2023,https://aclanthology.org/2023.acl-long.572.
"Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang. Editing large language models:
Problems, methods, and opportunities. In H. Bouamor, J. Pino, and K. Bali (eds.), Proceedings of the
2023 Conference on Empirical Methods in Natural Language Processing, pp. 10222–10240, Singapore,
December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.632. URL
https://aclanthology.org/2023.emnlp-main.632.",2023,https://aclanthology.org/2023.emnlp-main.632.
"K. Yin and G. Neubig. Interpreting language models with contrastive explanations. In Y. Goldberg, Z. Kozareva,
and Y. Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 184–198, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.emnlp-main.14. URL https://aclanthology.org/2022.emnlp-mai
n.14.",2022,"https://aclanthology.org/2022.emnlp-mai
n.14."
"Q. Yu, J. Merullo, and E. Pavlick. Characterizing mechanisms for factual recall in language models. In
H. Bouamor, J. Pino, and K. Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing, pp. 9924–9959, Singapore, December 2023a. Association for Computational
Linguistics. doi: 10.18653/v1/2023.emnlp-main.615. URL https://aclanthology.org/2023.emnlp-m
ain.615.",2023,"https://aclanthology.org/2023.emnlp-m
ain.615."
"Y. Yu, S. Buchanan, D. Pai, T. Chu, Z. Wu, S. Tong, B. D. Haeffele, and Y. Ma. White-box transformers via
sparse rate reduction. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b. URL
https://openreview.net/forum?id=THfl8hdVxH.",,https://openreview.net/forum?id=THfl8hdVxH.
"Z. Yu and S. Ananiadou. Locating factual knowledge in large language models: Exploring the residual stream
and analyzing subvalues in vocabulary space, 2024. URL https://arxiv.org/abs/2312.12141.",2024,https://arxiv.org/abs/2312.12141.
"M. Yuksekgonul, V. Chandrasekaran, E. Jones, S. Gunasekar, R. Naik, H. Palangi, E. Kamar, and B. Nushi.
Attention satisfies: A constraint-satisfaction lens on factual errors of language models. In The Twelfth
International Conference on Learning Representations, 2024. URL https://openreview.net/forum?i
d=gfFVATffPd.",2024,"https://openreview.net/forum?i
d=gfFVATffPd."
"M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In D. Fleet, T. Pajdla,
B. Schiele, and T. Tuytelaars (eds.), Computer Vision – ECCV 2014, pp. 818–833, Cham, 2014. Springer
International Publishing. ISBN 978-3-319-10590-1.",2014,
"B. Zhang and R. Sennrich. Root mean square layer normalization. In Proceedings of the 33rd International
Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2019. Curran Associates Inc.",2019,
"F. Zhang and N. Nanda. Towards best practices of activation patching in language models: Metrics and methods.
In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview
.net/forum?id=Hf17y6u9BC.",2024,"https://openreview
.net/forum?id=Hf17y6u9BC."
"S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov,
M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer. Opt: Open
pre-trained transformer language models, 2022.",2022,
"Z. Zhao and B. Shan. Reagent: A model-agnostic feature attribution method for generative language models,
2024.",2024,
"Z. Zhong, Z. Liu, M. Tegmark, and J. Andreas. The clock and the pizza: Two stories in mechanistic explanation
of neural networks. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.),
Advances in Neural Information Processing Systems, volume 36, pp. 27223–27250. Curran Associates, Inc.,
2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/56cbfbf49937a0873
d451343ddc8c57d-Paper-Conference.pdf.",2023,"https://proceedings.neurips.cc/paper_files/paper/2023/file/56cbfbf49937a0873
d451343ddc8c57d-Paper-Conference.pdf."
"B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Object detectors emerge in deep scene cnns. In
International Conference on Learning Representations (ICLR), 2015.",2015,
"H. Zhou, A. Bradley, E. Littwin, N. Razin, O. Saremi, J. M. Susskind, S. Bengio, and P. Nakkiran. What
algorithms can transformers learn? a study in length generalization. In The Twelfth International Conference
on Learning Representations, 2024. URL https://openreview.net/forum?id=AssIuHnmHX.",2024,https://openreview.net/forum?id=AssIuHnmHX.
"A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A.-K. Dombrowski,
S. Goel, N. Li, M. J. Byun, Z. Wang, A. Mallen, S. Basart, S. Koyejo, D. Song, M. Fredrikson, J. Z. Kolter,
and D. Hendrycks. Representation engineering: A top-down approach to ai transparency. Arxiv, 2023. URL
https://arxiv.org/abs/2310.01405.",2023,https://arxiv.org/abs/2310.01405.
