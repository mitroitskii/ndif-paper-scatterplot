row_id,rank,method,method_short,method_details,evaluation_date,metrics,raw_metrics,uses_additional_data,paper,tags,arc_accuracy
99242,1,"GPT-4 (few-shot, k=25)",GPT-4 ,"few-shot, k=25",2023-03-15,{'Accuracy': '96.4'},{'Accuracy': 96.4},False,"{'id': 1174373, 'title': 'GPT-4 Technical Report', 'url': '/paper/gpt-4-technical-report-1', 'published': '2023-03-15T00:00:00.000000', 'code': True, 'review_url': '/paper/gpt-4-technical-report-1/review/?hl=99242'}","[{'id': 183, 'name': 'few-shot', 'color': '#a1df95'}]",96.4
102617,2,"PaLM 2 (few-shot, CoT, SC)",PaLM 2 ,"few-shot, CoT, SC",2023-05-17,{'Accuracy': '95.1'},{'Accuracy': 95.1},False,"{'id': 1210556, 'title': 'PaLM 2 Technical Report', 'url': '/paper/palm-2-technical-report-1', 'published': '2023-05-17T00:00:00.000000', 'code': True, 'review_url': '/paper/palm-2-technical-report-1/review/?hl=102617'}","[{'id': 183, 'name': 'few-shot', 'color': '#a1df95'}]",95.1
117558,3,"Claude 2 (few-shot, k=5)",Claude 2 ,"few-shot, k=5",2023-07-11,{'Accuracy': '91'},{'Accuracy': 91.0},False,"{'id': 1300010, 'title': 'Model Card and Evaluations for Claude Models', 'url': '/paper/model-card-and-evaluations-for-claude-models', 'published': '2023-07-11T00:00:00.000000', 'code': False, 'review_url': None}",[],91.0
117557,4,"Claude 1.3 (few-shot, k=5)",Claude 1.3 ,"few-shot, k=5",2023-04-18,{'Accuracy': '90'},{'Accuracy': 90.0},False,"{'id': 1300010, 'title': 'Model Card and Evaluations for Claude Models', 'url': '/paper/model-card-and-evaluations-for-claude-models', 'published': '2023-07-11T00:00:00.000000', 'code': False, 'review_url': None}",[],90.0
102591,5,"PaLM 540B (Self Improvement, Self Consistency)",PaLM 540B ,"Self Improvement, Self Consistency",2022-10-20,{'Accuracy': '89.8'},{'Accuracy': 89.8},False,"{'id': 1097944, 'title': 'Large Language Models Can Self-Improve', 'url': '/paper/large-language-models-can-self-improve', 'published': '2022-10-20T00:00:00.000000', 'code': False, 'review_url': '/paper/large-language-models-can-self-improve/review/?hl=102591'}",[],89.8
102585,6,PaLM 540B (Self Consistency),PaLM 540B ,Self Consistency,2022-10-20,{'Accuracy': '88.7'},{'Accuracy': 88.7},False,"{'id': 1097944, 'title': 'Large Language Models Can Self-Improve', 'url': '/paper/large-language-models-can-self-improve', 'published': '2022-10-20T00:00:00.000000', 'code': False, 'review_url': '/paper/large-language-models-can-self-improve/review/?hl=102585'}",[],88.7
102589,7,"PaLM 540B (Self Improvement, CoT Prompting)",PaLM 540B ,"Self Improvement, CoT Prompting",2022-10-20,{'Accuracy': '88.3'},{'Accuracy': 88.3},False,"{'id': 1097944, 'title': 'Large Language Models Can Self-Improve', 'url': '/paper/large-language-models-can-self-improve', 'published': '2022-10-20T00:00:00.000000', 'code': False, 'review_url': '/paper/large-language-models-can-self-improve/review/?hl=102589'}",[],88.3
102587,8,"PaLM 540B (Self Improvement, Standard-Prompting)",PaLM 540B ,"Self Improvement, Standard-Prompting",2022-10-20,{'Accuracy': '87.2'},{'Accuracy': 87.2},False,"{'id': 1097944, 'title': 'Large Language Models Can Self-Improve', 'url': '/paper/large-language-models-can-self-improve', 'published': '2022-10-20T00:00:00.000000', 'code': False, 'review_url': '/paper/large-language-models-can-self-improve/review/?hl=102587'}",[],87.2
102581,9,PaLM 540B (Standard-Prompting),PaLM 540B ,Standard-Prompting,2022-10-20,{'Accuracy': '87.1'},{'Accuracy': 87.1},False,"{'id': 1097944, 'title': 'Large Language Models Can Self-Improve', 'url': '/paper/large-language-models-can-self-improve', 'published': '2022-10-20T00:00:00.000000', 'code': False, 'review_url': '/paper/large-language-models-can-self-improve/review/?hl=102581'}",[],87.1
118321,10,ST-MoE-32B 269B (fine-tuned),ST-MoE-32B 269B ,fine-tuned,2022-02-17,{'Accuracy': '86.5'},{'Accuracy': 86.5},False,"{'id': 964307, 'title': 'ST-MoE: Designing Stable and Transferable Sparse Expert Models', 'url': '/paper/designing-effective-sparse-expert-models', 'published': '2022-02-17T00:00:00.000000', 'code': True, 'review_url': '/paper/designing-effective-sparse-expert-models/review/?hl=118321'}",[],86.5
117556,11,"Claude Instant 1.1 (few-shot, k=5)",Claude Instant 1.1 ,"few-shot, k=5",2023-07-11,{'Accuracy': '85.7'},{'Accuracy': 85.7},False,"{'id': 1300010, 'title': 'Model Card and Evaluations for Claude Models', 'url': '/paper/model-card-and-evaluations-for-claude-models', 'published': '2023-07-11T00:00:00.000000', 'code': False, 'review_url': None}",[],85.7
99243,12,"GPT-3.5 (few-shot, k=25)",GPT-3.5 ,"few-shot, k=25",2023-03-15,{'Accuracy': '85.2'},{'Accuracy': 85.2},False,"{'id': 1174373, 'title': 'GPT-4 Technical Report', 'url': '/paper/gpt-4-technical-report-1', 'published': '2023-03-15T00:00:00.000000', 'code': True, 'review_url': '/paper/gpt-4-technical-report-1/review/?hl=99243'}","[{'id': 183, 'name': 'few-shot', 'color': '#a1df95'}]",85.2
102583,13,PaLM 540B (CoT Prompting),PaLM 540B ,CoT Prompting,2022-10-20,{'Accuracy': '85.2'},{'Accuracy': 85.2},False,"{'id': 1097944, 'title': 'Large Language Models Can Self-Improve', 'url': '/paper/large-language-models-can-self-improve', 'published': '2022-10-20T00:00:00.000000', 'code': False, 'review_url': '/paper/large-language-models-can-self-improve/review/?hl=102583'}",[],85.2
123798,14,LLaMA 3 8B + MoSLoRA (fine-tuned),LLaMA 3 8B + MoSLoRA ,fine-tuned,2024-06-16,{'Accuracy': '81.5'},{'Accuracy': 81.5},False,"{'id': 1466105, 'title': 'Mixture-of-Subspaces in Low-Rank Adaptation', 'url': '/paper/mixture-of-subspaces-in-low-rank-adaptation', 'published': '2024-06-16T00:00:00.000000', 'code': True, 'review_url': '/paper/mixture-of-subspaces-in-low-rank-adaptation/review/?hl=123798'}",[],81.5
122579,15,LLaMA-3 8B + MixLoRA,LLaMA-3 8B + MixLoRA,,2024-04-22,{'Accuracy': '79.9'},{'Accuracy': 79.9},False,"{'id': 1426089, 'title': 'MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts', 'url': '/paper/mixlora-enhancing-large-language-models-fine', 'published': '2024-04-22T00:00:00.000000', 'code': True, 'review_url': '/paper/mixlora-enhancing-large-language-models-fine/review/?hl=122579'}",[],79.9
122589,16,LLaMA-2 13B + MixLoRA,LLaMA-2 13B + MixLoRA,,2024-04-22,{'Accuracy': '69.9'},{'Accuracy': 69.9},False,"{'id': 1426089, 'title': 'MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts', 'url': '/paper/mixlora-enhancing-large-language-models-fine', 'published': '2024-04-22T00:00:00.000000', 'code': True, 'review_url': '/paper/mixlora-enhancing-large-language-models-fine/review/?hl=122589'}",[],69.9
118920,17,PaLM 2-L (1-shot),PaLM 2-L ,1-shot,2023-05-17,{'Accuracy': '69.2'},{'Accuracy': 69.2},False,"{'id': 1210556, 'title': 'PaLM 2 Technical Report', 'url': '/paper/palm-2-technical-report-1', 'published': '2023-05-17T00:00:00.000000', 'code': True, 'review_url': '/paper/palm-2-technical-report-1/review/?hl=118920'}",[],69.2
85070,18,GAL 120B (zero-shot),GAL 120B ,zero-shot,2022-11-16,{'Accuracy': '67.9'},{'Accuracy': 67.9},True,"{'id': 1112728, 'title': 'Galactica: A Large Language Model for Science', 'url': '/paper/galactica-a-large-language-model-for-science-1', 'published': '2022-11-16T00:00:00.000000', 'code': True, 'review_url': '/paper/galactica-a-large-language-model-for-science-1/review/?hl=85070'}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",67.9
119096,19,Camelidae-8×34B,Camelidae-8×34B,,2024-01-05,{'Accuracy': '65.2'},{'Accuracy': 65.2},False,"{'id': 1355944, 'title': 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks', 'url': '/paper/parameter-efficient-sparsity-crafting-from', 'published': '2024-01-05T00:00:00.000000', 'code': True, 'review_url': '/paper/parameter-efficient-sparsity-crafting-from/review/?hl=119096'}",[],65.2
118919,20,PaLM 2-M (1-shot),PaLM 2-M ,1-shot,2023-05-17,{'Accuracy': '64.9'},{'Accuracy': 64.9},False,"{'id': 1210556, 'title': 'PaLM 2 Technical Report', 'url': '/paper/palm-2-technical-report-1', 'published': '2023-05-17T00:00:00.000000', 'code': True, 'review_url': '/paper/palm-2-technical-report-1/review/?hl=118919'}",[],64.9
118132,21,"FLAN 137B (few-shot, k=13)",FLAN 137B ,"few-shot, k=13",2021-09-03,{'Accuracy': '63.8'},{'Accuracy': 63.8},False,"{'id': 861409, 'title': 'Finetuned Language Models Are Zero-Shot Learners', 'url': '/paper/finetuned-language-models-are-zero-shot', 'published': '2021-09-03T00:00:00.000000', 'code': True, 'review_url': '/paper/finetuned-language-models-are-zero-shot/review/?hl=118132'}",[],63.8
118135,22,FLAN 137B (zero-shot),FLAN 137B ,zero-shot,2021-09-03,{'Accuracy': '63.1'},{'Accuracy': 63.1},False,"{'id': 861409, 'title': 'Finetuned Language Models Are Zero-Shot Learners', 'url': '/paper/finetuned-language-models-are-zero-shot', 'published': '2021-09-03T00:00:00.000000', 'code': True, 'review_url': '/paper/finetuned-language-models-are-zero-shot/review/?hl=118135'}",[],63.1
118918,23,PaLM 2-S (1-shot),PaLM 2-S ,1-shot,2023-05-17,{'Accuracy': '59.6'},{'Accuracy': 59.6},False,"{'id': 1210556, 'title': 'PaLM 2 Technical Report', 'url': '/paper/palm-2-technical-report-1', 'published': '2023-05-17T00:00:00.000000', 'code': True, 'review_url': '/paper/palm-2-technical-report-1/review/?hl=118918'}",[],59.6
122571,24,LLaMA-2 7B + MixLoRA,LLaMA-2 7B + MixLoRA,,2024-04-22,{'Accuracy': '58.1'},{'Accuracy': 58.1},False,"{'id': 1426089, 'title': 'MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts', 'url': '/paper/mixlora-enhancing-large-language-models-fine', 'published': '2024-04-22T00:00:00.000000', 'code': True, 'review_url': '/paper/mixlora-enhancing-large-language-models-fine/review/?hl=122571'}",[],58.1
97631,25,LLaMA 33B (zero-shot),LLaMA 33B ,zero-shot,2023-02-27,{'Accuracy': '57.8'},{'Accuracy': 57.8},False,"{'id': 1164350, 'title': 'LLaMA: Open and Efficient Foundation Language Models', 'url': '/paper/llama-open-and-efficient-foundation-language-1', 'published': '2023-02-27T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-open-and-efficient-foundation-language-1/review/?hl=97631'}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",57.8
118332,26,ST-MoE-L 4.1B (fine-tuned),ST-MoE-L 4.1B ,fine-tuned,2022-02-17,{'Accuracy': '56.9'},{'Accuracy': 56.9},False,"{'id': 964307, 'title': 'ST-MoE: Designing Stable and Transferable Sparse Expert Models', 'url': '/paper/designing-effective-sparse-expert-models', 'published': '2022-02-17T00:00:00.000000', 'code': True, 'review_url': '/paper/designing-effective-sparse-expert-models/review/?hl=118332'}",[],56.9
97632,27,LLaMA 65B (zero-shot),LLaMA 65B ,zero-shot,2023-02-27,{'Accuracy': '56.0'},{'Accuracy': 56.0},True,"{'id': 1164350, 'title': 'LLaMA: Open and Efficient Foundation Language Models', 'url': '/paper/llama-open-and-efficient-foundation-language-1', 'published': '2023-02-27T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-open-and-efficient-foundation-language-1/review/?hl=97632'}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",56.0
118940,28,Mistral 7B (0-shot),Mistral 7B ,0-shot,2023-10-10,{'Accuracy': '55.5'},{'Accuracy': 55.5},False,"{'id': 1297015, 'title': 'Mistral 7B', 'url': '/paper/mistral-7b', 'published': '2023-10-10T00:00:00.000000', 'code': True, 'review_url': '/paper/mistral-7b/review/?hl=118940'}",[],55.5
60408,29,GPT-3 175B (1 shot),GPT-3 175B ,1 shot,2020-05-28,{'Accuracy': '53.2'},{'Accuracy': 53.2},True,"{'id': 198147, 'title': 'Language Models are Few-Shot Learners', 'url': '/paper/language-models-are-few-shot-learners', 'published': '2020-05-28T00:00:00.000000', 'code': True, 'review_url': '/paper/language-models-are-few-shot-learners/review/?hl=60408'}","[{'id': 214, 'name': 'one-shot', 'color': '#ea9e57'}]",53.2
97630,30,LLaMA 13B (zero-shot),LLaMA 13B ,zero-shot,2023-02-27,{'Accuracy': '52.7'},{'Accuracy': 52.7},False,"{'id': 1164350, 'title': 'LLaMA: Open and Efficient Foundation Language Models', 'url': '/paper/llama-open-and-efficient-foundation-language-1', 'published': '2023-02-27T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-open-and-efficient-foundation-language-1/review/?hl=97630'}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",52.7
85097,31,GPT-3 (zero-shot),GPT-3 ,zero-shot,2022-11-16,{'Accuracy': '51.4'},{'Accuracy': 51.4},False,"{'id': 1112728, 'title': 'Galactica: A Large Language Model for Science', 'url': '/paper/galactica-a-large-language-model-for-science-1', 'published': '2022-11-16T00:00:00.000000', 'code': True, 'review_url': '/paper/galactica-a-large-language-model-for-science-1/review/?hl=85097'}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",51.4
117927,32,GPT-3 175B (0-shot),GPT-3 175B ,0-shot,2020-05-28,{'Accuracy': '51.4'},{'Accuracy': 51.4},False,"{'id': 198147, 'title': 'Language Models are Few-Shot Learners', 'url': '/paper/language-models-are-few-shot-learners', 'published': '2020-05-28T00:00:00.000000', 'code': True, 'review_url': '/paper/language-models-are-few-shot-learners/review/?hl=117927'}",[],51.4
118962,33,BLOOM 176B (1-shot),BLOOM 176B ,1-shot,2023-03-30,{'Accuracy': '50.85'},{'Accuracy': 50.85},False,"{'id': 1183339, 'title': 'BloombergGPT: A Large Language Model for Finance', 'url': '/paper/bloomberggpt-a-large-language-model-for', 'published': '2023-03-30T00:00:00.000000', 'code': False, 'review_url': '/paper/bloomberggpt-a-large-language-model-for/review/?hl=118962'}",[],50.85
60409,34,GLaM 64B/64E (0 shot),GLaM 64B/64E ,0 shot,2021-12-13,{'Accuracy': '50.3'},{'Accuracy': 50.3},True,"{'id': 929896, 'title': 'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts', 'url': '/paper/glam-efficient-scaling-of-language-models', 'published': '2021-12-13T00:00:00.000000', 'code': False, 'review_url': '/paper/glam-efficient-scaling-of-language-models/review/?hl=60409'}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",50.3
118376,35,UL2 20B (chain-of-thought + self-consistency),UL2 20B ,chain-of-thought + self-consistency,2022-05-10,{'Accuracy': '49.5'},{'Accuracy': 49.5},False,"{'id': 1007751, 'title': 'UL2: Unifying Language Learning Paradigms', 'url': '/paper/unifying-language-learning-paradigms', 'published': '2022-05-10T00:00:00.000000', 'code': True, 'review_url': '/paper/unifying-language-learning-paradigms/review/?hl=118376'}",[],49.5
118960,36,Bloomberg GPT 50B (1-shot),Bloomberg GPT 50B ,1-shot,2023-03-30,{'Accuracy': '48.63'},{'Accuracy': 48.63},False,"{'id': 1183339, 'title': 'BloombergGPT: A Large Language Model for Finance', 'url': '/paper/bloomberggpt-a-large-language-model-for', 'published': '2023-03-30T00:00:00.000000', 'code': False, 'review_url': '/paper/bloomberggpt-a-large-language-model-for/review/?hl=118960'}",[],48.63
60410,37,GLaM 64B/64E (1 shot),GLaM 64B/64E ,1 shot,2021-12-13,{'Accuracy': '48.2'},{'Accuracy': 48.2},True,"{'id': 929896, 'title': 'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts', 'url': '/paper/glam-efficient-scaling-of-language-models', 'published': '2021-12-13T00:00:00.000000', 'code': False, 'review_url': '/paper/glam-efficient-scaling-of-language-models/review/?hl=60410'}","[{'id': 214, 'name': 'one-shot', 'color': '#ea9e57'}]",48.2
97629,38,LLaMA 7B (zero-shot),LLaMA 7B ,zero-shot,2023-02-27,{'Accuracy': '47.6'},{'Accuracy': 47.6},False,"{'id': 1164350, 'title': 'LLaMA: Open and Efficient Foundation Language Models', 'url': '/paper/llama-open-and-efficient-foundation-language-1', 'published': '2023-02-27T00:00:00.000000', 'code': True, 'review_url': '/paper/llama-open-and-efficient-foundation-language-1/review/?hl=97629'}","[{'id': 188, 'name': 'zero-shot', 'color': '#2771D3'}]",47.6
118961,39,GPT-NeoX 20B (1-shot),GPT-NeoX 20B ,1-shot,2023-03-30,{'Accuracy': '45.39'},{'Accuracy': 45.39},False,"{'id': 1183339, 'title': 'BloombergGPT: A Large Language Model for Finance', 'url': '/paper/bloomberggpt-a-large-language-model-for', 'published': '2023-03-30T00:00:00.000000', 'code': False, 'review_url': '/paper/bloomberggpt-a-large-language-model-for/review/?hl=118961'}",[],45.39
108557,40,phi-1.5-web 1.3B (zero-shot),phi-1.5-web 1.3B ,zero-shot,2023-09-11,{'Accuracy': '44.9'},{'Accuracy': 44.9},False,"{'id': 1275377, 'title': 'Textbooks Are All You Need II: phi-1.5 technical report', 'url': '/paper/textbooks-are-all-you-need-ii-phi-1-5', 'published': '2023-09-11T00:00:00.000000', 'code': True, 'review_url': '/paper/textbooks-are-all-you-need-ii-phi-1-5/review/?hl=108557'}",[],44.9
100770,41,OPT 66B (one-shot),OPT 66B ,one-shot,2023-03-30,{'Accuracy': '44.54'},{'Accuracy': 44.54},False,"{'id': 1183339, 'title': 'BloombergGPT: A Large Language Model for Finance', 'url': '/paper/bloomberggpt-a-large-language-model-for', 'published': '2023-03-30T00:00:00.000000', 'code': False, 'review_url': '/paper/bloomberggpt-a-large-language-model-for/review/?hl=100770'}","[{'id': 214, 'name': 'one-shot', 'color': '#ea9e57'}]",44.54
88786,42,OPT-175B,OPT-175B,,2023-01-02,{'Accuracy': '43.94'},{'Accuracy': 43.94},False,"{'id': 1136959, 'title': 'SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot', 'url': '/paper/massive-language-models-can-be-accurately', 'published': '2023-01-02T00:00:00.000000', 'code': True, 'review_url': '/paper/massive-language-models-can-be-accurately/review/?hl=88786'}",[],43.94
118373,43,UL2 20B (chain-of-thought),UL2 20B ,chain-of-thought,2022-05-10,{'Accuracy': '42.9'},{'Accuracy': 42.9},False,"{'id': 1007751, 'title': 'UL2: Unifying Language Learning Paradigms', 'url': '/paper/unifying-language-learning-paradigms', 'published': '2022-05-10T00:00:00.000000', 'code': True, 'review_url': '/paper/unifying-language-learning-paradigms/review/?hl=118373'}",[],42.9
88789,44,"SparseGPT (175B, 50% Sparsity)",SparseGPT ,"175B, 50% Sparsity",2023-01-02,{'Accuracy': '41.3'},{'Accuracy': 41.3},False,"{'id': 1136959, 'title': 'SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot', 'url': '/paper/massive-language-models-can-be-accurately', 'published': '2023-01-02T00:00:00.000000', 'code': True, 'review_url': '/paper/massive-language-models-can-be-accurately/review/?hl=88789'}",[],41.3
88790,45,"SparseGPT (175B, 4:8 Sparsity)",SparseGPT ,"175B, 4:8 Sparsity",2023-01-02,{'Accuracy': '39.85'},{'Accuracy': 39.85},False,"{'id': 1136959, 'title': 'SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot', 'url': '/paper/massive-language-models-can-be-accurately', 'published': '2023-01-02T00:00:00.000000', 'code': True, 'review_url': '/paper/massive-language-models-can-be-accurately/review/?hl=88790'}",[],39.85
88791,46,"SparseGPT (175B, 2:4 Sparsity)",SparseGPT ,"175B, 2:4 Sparsity",2023-01-02,{'Accuracy': '38.99'},{'Accuracy': 38.99},False,"{'id': 1136959, 'title': 'SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot', 'url': '/paper/massive-language-models-can-be-accurately', 'published': '2023-01-02T00:00:00.000000', 'code': True, 'review_url': '/paper/massive-language-models-can-be-accurately/review/?hl=88791'}",[],38.99
120090,47,Pythia 12B (5-shot),Pythia 12B ,5-shot,2023-04-03,{'Accuracy': '36.8'},{'Accuracy': 36.8},False,"{'id': 1186187, 'title': 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling', 'url': '/paper/pythia-a-suite-for-analyzing-large-language', 'published': '2023-04-03T00:00:00.000000', 'code': True, 'review_url': '/paper/pythia-a-suite-for-analyzing-large-language/review/?hl=120090'}",[],36.8
85096,48,"BLOOM (few-shot, k=5)",BLOOM ,"few-shot, k=5",2022-11-16,{'Accuracy': '32.9'},{'Accuracy': 32.9},False,"{'id': 1112728, 'title': 'Galactica: A Large Language Model for Science', 'url': '/paper/galactica-a-large-language-model-for-science-1', 'published': '2022-11-16T00:00:00.000000', 'code': True, 'review_url': '/paper/galactica-a-large-language-model-for-science-1/review/?hl=85096'}","[{'id': 183, 'name': 'few-shot', 'color': '#a1df95'}]",32.9
120083,49,Pythia 12B (0-shot),Pythia 12B ,0-shot,2023-04-03,{'Accuracy': '31.8'},{'Accuracy': 31.8},False,"{'id': 1186187, 'title': 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling', 'url': '/paper/pythia-a-suite-for-analyzing-large-language', 'published': '2023-04-03T00:00:00.000000', 'code': True, 'review_url': '/paper/pythia-a-suite-for-analyzing-large-language/review/?hl=120083'}",[],31.8
85095,50,"OPT (few-shot, k=5)",OPT ,"few-shot, k=5",2022-11-16,{'Accuracy': '31.1'},{'Accuracy': 31.1},False,"{'id': 1112728, 'title': 'Galactica: A Large Language Model for Science', 'url': '/paper/galactica-a-large-language-model-for-science-1', 'published': '2022-11-16T00:00:00.000000', 'code': True, 'review_url': '/paper/galactica-a-large-language-model-for-science-1/review/?hl=85095'}","[{'id': 183, 'name': 'few-shot', 'color': '#a1df95'}]",31.1
118370,51,UL2 20B (zero-shot),UL2 20B ,zero-shot,2022-05-10,{'Accuracy': '29.8'},{'Accuracy': 29.8},False,"{'id': 1007751, 'title': 'UL2: Unifying Language Learning Paradigms', 'url': '/paper/unifying-language-learning-paradigms', 'published': '2022-05-10T00:00:00.000000', 'code': True, 'review_url': '/paper/unifying-language-learning-paradigms/review/?hl=118370'}",[],29.8
88788,52,OPT-175B (50% Sparsity),OPT-175B ,50% Sparsity,2023-01-02,{'Accuracy': '25.6'},{'Accuracy': 25.6},False,"{'id': 1136959, 'title': 'SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot', 'url': '/paper/massive-language-models-can-be-accurately', 'published': '2023-01-02T00:00:00.000000', 'code': True, 'review_url': '/paper/massive-language-models-can-be-accurately/review/?hl=88788'}",[],25.6
